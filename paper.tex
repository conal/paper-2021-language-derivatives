\documentclass[hidelinks,12pt]{article}  % fleqn,12pt
\usepackage[margin=0.2in]{geometry}  % 0.12in, 0.9in, 1in

\usepackage{catchfilebetweentags}
\usepackage[useregional]{datetime2}

%% \usepackage{balance}  % even out final two-column page

\RequirePackage{newunicodechar, amssymb, stmaryrd, unicode-math, setspace, comment}

\input{commands}
\input{unicode}
\input{macros}

\usepackage{libertine}  %% [tt=false]
\usepackage{agda}% references

%% Switching to \textsf for AgdaFunction messes up vertical alignment.
%% \newcommand{\AgdaFontStyle}[1]{\textsf{#1}}

%% \renewcommand{\AgdaFontStyle}[1]{\text{#1}}

%% \renewcommand{\AgdaFunction}[1]
%%     {\AgdaNoSpaceMath{\text{\textcolor{AgdaFunction}{\AgdaFormat{#1}{\AgdaLink{#1}}}}}}
%% \renewcommand{\AgdaRecord}[1]
%%     {\AgdaNoSpaceMath{\text{\textcolor{AgdaRecord}{\AgdaFormat{#1}{\AgdaLink{#1}}}}}}

%% \rnc\AgdaTarget[1]{}

\author{Conal Elliott}

\begin{document}

%% \nc\tit{Working Title}
\nc\tit{%Functional Pearl:
Symbolic and Automatic Differentiation of Languages}

\title{\tit}
\date{Early draft of \DTMnow}

\maketitle

\begin{abstract}
Formal languages are usually defined in terms of set theory.
Choosing type theory instead gives us languages as type-level predicates over strings.
Applying a language to a string yields a type whose elements are language membership proofs describing \emph{how} a string parses in the language.
The usual building blocks of languages (including union, contatenation, and Kleene closure) have precise and compelling specifications uncomplicated by operational strategies and are easily generalized to a few general domain-transforming and codomain-transforming operations on predicates.

A simple characterization of languages (and indeed functions from lists to any type) captures the essential idea behind language ``differentiation'' and ``integration'' as used for recognizing languages and leads to a collection of lemmas about type-level languages.
These lemmas form the backbone of two dual implementations of language recognition---(inductive) regular expressions and (coinductive) tries---each containing the same code but in dual arrangements.
The language lemmas form most of the implementation in both cases, while the representation and primitive operations trade places.
The regular expression version corresponds to symbolic differentiation, while the trie version corresponds to automatic differentiation.

The relatively easy-to-prove properties of type-level languages transfer almost effortlessly to the decidable implementations.
In particular, despite the inductive and coinductive nature of regular expressions and tries respectively, we need neither inductive nor coinductive/bisumulation arguments to prove algebraic properties.
\end{abstract}

\sectionl{Specifying Languages}
\rnc\source{Language}

Languages are usually formalized either set-theoretically as a set of strings \needcite{} or operationally as a parser \needcite{}.
Alternatively, we can use type theory, so that a language is a type-level predicate on ``strings'' (lists of an arbitrary type \AB{A} of ``characters''):
\agda{Lang}
\figrefdef{Lang-ops}{Language operations}{\agda{Lang-ops}} gives definitions of the usual language operations, including language union and intersection ({\AB{P} \AF ‚à™ \AB{Q}} and {\AB{P} \AF ‚à© \AB{Q}}) with their identities (the empty language \AF ‚àÖ and universal language \AF ùí∞), language concatenation ({\AB{P} \AF ‚ãÜ \AB{Q}}) and its identity (\AF ùüè, containing only the empty string), single-character languages ({\AF ` \AB{c}}), Kleene star ({\AB{P} \AF ‚òÜ}), and ``scalar multiplication'' (\mbox{\AB{s} \AF ¬∑ \AB{P}}, which will prove useful later).

\note{Explain the types used here and their logical interpretation under the Curry-Howard isomorphism.}

Note that for a language \AB{P} and string \AB{w}, {\AB{P} \AB{w}} is the type of \emph{proofs} that {\AB{w} ‚àà \AB{P}}, in other words \emph{explanations} of membership, or \emph{parsing}.
(If the type {\AB{P} \AB{w}} is uninhabited, then {\AB{w} ‚àâ \AB{P}}.)
For instance, a proof of {\AB{w} ‚àà \AB{P} \AF ‚à™ \AB{Q}} contains a proof of {\AB{w} ‚àà \AB{P}} or of {\AB{w} \AF ‚àà \AB{Q}}, \emph{and} the knowledge of which one was chosen.
Likewise, a proof of {w ‚àà \AB{P} \AF ‚ãÜ \AB{Q}} (language concatenation) includes the choice of strings \AB{u} and \AB{v}, a proof that {\AB{u} \AF ‚äô \AB{v} \AD ‚â° \AB{w}}, and proofs that {\AB{u} ‚àà \AB{P}} and {\AB{v} ‚àà \AB{P}}.

The use of type-level predicates makes for simple, direct specification, including the use of existential quantification.
As described in \secref{Properties}, it's also fairly easy to prove algebraic properties of predicates\out{, including the semimodule and semiring laws for both predicate semirings}.
These definitions do not, however, give us decidable parsing, because general type inhabitation amounts to theorem proving.


\sectionl{Decomposing Languages}
\rnc\source{Calculus}

In order to convert languages into parsers, it will help to decompose languages---and indeed any function from lists---into a regular form.
Since lists are defined by an algebraic data type, we can decompose languages according to constructor, as follows:
\agda{ŒΩŒ¥}
Each use of \AF{Œ¥} takes us one step closer to reducing general language membership to nullability.
In other words,\footnote{Repeated application is expressed as a standard left fold\out{ \stdlibCitep{Data.List}}:
\agda{foldl}
}%
\agda{ŒΩ‚àòfoldlŒ¥}

This simple observation is the semantic heart of the syntactic technique of \emph{derivatives of regular expressions} as used for efficient recognition of regular languages \citep{Brzozowski64}, later extended to parsing general context-free languages \citep{Might2010YaccID}.
Lemma \AF{ŒΩ‚àòfoldlŒ¥} liberates this technique from the assumption that languages are represented \emph{symbolically}, by some form of grammar (e.g., regular or context-free), inviting other representations as we will see in \secref{Automatic Differentiation}.

Lemma \AF{ŒΩ‚àòfoldlŒ¥} relates to some other powerful perspectives and principles as well:
\begin{itemize}

\item 
\emph{Automata theory}: {\AF{ŒΩ} \AF ‚àò \AF{foldl} \AF{Œ¥} \AB{P}} is the execution of a state machine.
Each state is a language, with \AB{P} being the initial state; \AF{Œ¥} is the state transition function; and \AB{ŒΩ} is the set of accepting states \needcite{}.
Lemma \AF{ŒΩ‚àòfoldlŒ¥} captures the correctness of this state machine as a recognizer for the language \AB{P}.

\item
\emph{Incremental computation}: {\AF{ŒΩ} \AF ‚àò \AF{foldl} \AF{Œ¥} \AB{P}} computes \AB{P} by transforming successive incremental modifications to the empty string (each in the form of {\AB{a} \AIC ‚à∑\_} for a list element \AB{a}) into incremental modifications to the membership question for the related language {\AF{Œ¥} \AB{P} \AB{a}} \needcite{}.
Lemma \AF{ŒΩ‚àòfoldlŒ¥} again captures the correctness of this incremental implementation of \AB{P}.

\item 
\emph{Calculus}: \AB{Œ¥} is differentiation; {\AF{ŒΩ} \AF ‚àò \AF{foldl} \AF{Œ¥} \AB{P}} is integration; and lemma \AF{ŒΩ‚àòfoldlŒ¥} is the second fundamental theorem of calculus, which says that a function can be recovered by integrating its derivative \needcite{}.

\end{itemize}

Given the definitions of \AF{ŒΩ} and \AF{Œ¥} above and of the language operations in \secref{Specifying Languages}, one can prove properties about how they relate, as shown in
\figrefdef{nu-delta-lemmas}{Properties of \AF{ŒΩ} and \AF{Œ¥} for language operations}{\agda{ŒΩŒ¥-lemmas}}.
The correct-by-construction parsing algorithms in \secreftwo{Symbolic Differentiation}{Automatic Differentiation} are corollaries of these properties.

There are three relations involved in \figref{nu-delta-lemmas}: propositional equality (``‚â°''), (type) isomorphism (``‚Üî'') and extensional isomorphism (``‚ü∑'').
Isomorphism relates types (propositions) whose inhabitants (proofs) are in one-to-one correspondence.
\emph{Extensional} (or ``pointwise'') isomorphism relates predicates isomorphic on every argument:
\ExecuteMetaData[Inverses.tex]{ext-iso}

As with all lemmas in this paper shown with signatures only, full proofs are in the paper's source code and are formally verified by the Agda compiler.
The equalities in \figref{nu-delta-lemmas} are all proved automatically by normalization (i.e., their proofs are simply \AIC{refl}), while the other relations require a bit more work.


\sectionl{Decidability}
\rnc\source{Decidability}

For an effective implementation, we will have to bridge the gap between a type (of membership proofs) and its decidable inhabitation.
Fortunately, there is a convenient and compositional way to do so.
For any type \AB{A}, the type {\AF{Dec} \AB{A}} contains proof of \AB{A} or a proof of {\AF ¬¨ \AB{A}} (defined as usual to mean {\AB{A} \AS ‚Üí \AD{‚ä•}}):\footnote{This simple \AF{Dec} definition been superceded by a more efficient but more complex version \stdlibCitep{Relation.Nullary}.}
\agda{Dec}
For compositionality, we will use a few operations that lift decidability of types to decidability of constructions on those types, as shown in \figrefdef{compositional-dec}{Compositional decidability}{\agda{compositional-dec}}.\notefoot{Explanatory footnote goes here.}
Moreover, decidability lifts naturally from types to predicates:
\agda{Decidable}
With these definitions, we can formulate the problem of decidable language recognition: given a language \AB{P}, construct a term of type {\AF{Decidable} \AB{P}}.
Since this transformation cannot be fully automated, we will instead look for decidable building blocks that mirror the predicate vocabulary defined in \secref{Specifying Languages}.

Type isomorphisms (such as those in \figref{nu-delta-lemmas}) play an important role in decidability, namely that isomorphic types or predicates are equivalently decidable:
\agda{isomorphisms}
One direction of the isomorphism proves \AB{B} from \AB{A}, while the other proves {\AF ¬¨ B} from {\AF ¬¨ A}.
In fact, logical \emph{equivalence} suffices for the results in this paper, but the stronger condition of isomorphism enables variations such as generating all parses/proofs rather than just one.

\sectionl{Reflections}

The lemmas in \figref{nu-delta-lemmas} tell us how to decompose languages defined in terms of the vocabulary from \figref{Lang-ops}, while the definitions in \figref{compositional-dec} tell us how compute inhabitation of the resulting types, resulting in decidable parsing.
These lemmas and definitions cannot be applied \emph{automatically} in their present form, however, because languages are functions, as are \AF{ŒΩ} and \AF{Œ¥}, and so are not subject to pattern-matching.
An automatic solution would need some form of \emph{reflection} of language operations or \AF{ŒΩ} and \AF{Œ¥} as inspectable data.

This situation is exactly as in differential calculus, since differentiation in that setting is also defined on functions rather than on symbolic representations.
Fortunately, there are two standard solutions for \emph{computable} differentiation, going by the names of ``symbolic'' and ``automatic'' differentiation.\out{\footnote{There is also an incorrect (approximate) variation often referred to as ``numeric'' differentiation.}}
In the former, functions are represented symbolically in some suitable vocabulary, and pattern-matching is used to apply rules of differentiation to those representations.
The latter solution involves re-interpreting the vocabulary of functions to construct not just the usual functions, but also their derivatives \needcite{}.
The latter is often much more efficient than the former, since it easily avoids a good deal of redundant computation due to significant amounts of duplicated work between function ``primals'' and their derivatives.

\secreftwo{Symbolic Differentiation}{Automatic Differentiation} apply the symbolic and automatic strategies respectively to languages.
In both cases, the algorithms are proved correct with minimal effort as corollaries of the lemmas in \figref{nu-delta-lemmas}.

\sectionl{Symbolic Differentiation}
\rnc\source{Symbolic}

\nc\api{\setstretch{1.1}\agda{api}}

The ``symbolic differentiation'' style reflects language-building vocabulary (\figref{Lang-ops}) into an inductive data type (of modestly extended regular expressions), while re-defining \AF{ŒΩ} and \AF{Œ¥} as functions that pattern-match on that data type.
For convenience, use the same names as in \secref{Specifying Languages} for these new counterparts, while referring to the original versions via the module prefix ``{‚ó¨.}\hspace{0.05em}''.
The result is shown in \figrefdef{symbolic-api}{Regular expressions (inductive)}{\api} along with a semantic function {\AF{‚ü¶\_‚üß}} that converts a symbolic language representation to decidable form.

Decidable recognition (via {\AF{‚ü¶\_‚üß}}) relies only on \AF{ŒΩ} and \AF{Œ¥}, which are defined in \figrefdef{symbolic-defs}{Symbolic differentiation (column-major/patterns)}{\agda{defs}}, as systematically derived from the lemmas of \figref{nu-delta-lemmas}.\notefoot{Describe this derivation.}
These definitions are meant to be read in ``column-major'' order, i.e., each column is one function definition.
Correctness is guaranteed by the types of \AF{ŒΩ} and \AF{Œ¥} and so is proved automatically by type-checking \figref{symbolic-defs}.

\sectionl{Automatic Differentiation}
\rnc\source{Automatic}

\agda{rules}

\secref{Symbolic Differentiation} embodies one choice of reflecting functions into a data representation.
Now consider the dual strategy of ``automatic differentiation''.
This time, reflect \AF{ŒΩ} and \AF{Œ¥} into a \emph{coinductive} data type (of tries \needcite{}), while redefining the language-building vocabulary as functions on that data type defined by \emph{copatterns} \citep{AbelPientka2016}.
Again, we will use the same names as in \secref{Specifying Languages}, referring to the original versions via the module prefix ``{‚ó¨.}\hspace{0.05em}''.
The result is shown in \figrefdef{automatic-api}{Tries (coinductive)}{\api}.

Decidable recognition again relies only on \AF{ŒΩ} and \AF{Œ¥}, which are defined in \figrefdef{automatic-defs}{Automatic differentiation (row-major/copatterns)}{\agda{defs}}.
These definitions are meant to be read in ``row-major'' order, i.e., each row is one definition.
Note that the definitions are syntactically identical to those in \figref{symbolic-defs} but are organized dually.
(The compiler-generated syntax coloring differs to reflect the changed interpretation of the definitions.)

The correctness proof is \emph{almost} accomplished by type-checking, but there is a technical problem.
The \AF{Œ¥} clause for {\AB{p} \AF ‚ãÜ \AB{q}} does not satisfy Agda's termination checker, which cannot see that the argument {\AF{Œ¥} \AB{p} \AB{a}} in the recursive use of \AF{\_‚ãÜ\_} is in some sense smaller than \AB{p}.
Fortunately, this issue was already identified and solved by Andreas \citet{Abel2016}---also in the setting of trie-based language recognition---by using \emph{sized types} \citep{Abel2008, AbelPientka2016}.
\rnc\source{SizedAutomatic}
This solution needs only to give \AF{Lang} (now tries) an index {\AB{i} \AK : \APT{Size}} corresponding to the maximum depth to which a trie can be searched, or equivalently the longest string that can be matched\out{ (via {\AF{‚ü¶\_‚üß}})}.
In practice, we will work with arbitrarily deep tries, i.e., ones having index \APo{‚àû}, as in the type of {\AF{‚ü¶\_‚üß}}.
The modified representation is shown in \figrefdef{sized automatic-api}{Sized tries (coinductive)}{\api}.

Decidable recognition is defined exactly as with unsized tries (\figref{automatic-defs}), and this time the compiler successfully proves \emph{total} correctness (including termination).

\begin{comment}

\sectionl{Properties}

\subsectionl{Predicate Algebra}

The basic building blocks of type-level predicates---and languages in particular---form the vocabulary of a \emph{closed semiring} in two different ways\out{, as reflected in the structure of regular expressions \needcite{}}.
The semiring abstraction has three aspects: (a) a commutative monoid providing ``zero'' and ``addition'', (b) a (possibly non-commutative) monoid providing ``one'' and ``multiplication'', and (c) the relationship between them, namely that multiplication distributes over addition and zero.\footnote{Distributing of multiplication over zero is also known as ``annihilation''.}
In the first predicate semiring, which is \emph{commutative} (i.e., multiplication commutes), zero and addition are \AF ‚àÖ and \AF{\_‚à™\_}, while one and multiplication are \AF ùí∞ and \AF{\_‚à©\_}.

Closure adds the star operation.
Following existing conventions \stdlibCitep{Algebra.Structures}, extend the semiring operations and laws with star and its law, parametrized over an arbitrary type \AB{A} and equivalence relation \AB{\_‚âà\_}:\notefoot{To do: add (right-associated) \AF{star ≥}.}
\ExecuteMetaData[Closed/Structures.tex]{closed}
The \AF{ClosedSemiring} algebraic structure is similarly extended to \AF{ClosedCommutativeSemiring}.

Conveniently, booleans and types form commutative semirings with all necessary definitions already in the standard library.
(The equivalence relation used for types is isomorphism rather than equality.)
Both are also closed.
For booleans, closure maps both \AIC{false} and \AIC{true} to \AIC{true}, with the \AF{starÀ°} law holding definitionally.
For types, the closure of \AB{A} is {\AB{A} ‚ú∂} (the usual inductive list type) with a simple, non-inductive proof of \AF{starÀ°}.

\begin{code}[hide]
open import Closed ; open Closed.Types {‚Ñì}
\end{code}

This first closed semiring for predicates follows from a much more general pattern.
Given any two types \AB{A} and \AB{B}, if \AB{B} is a monoid then {\AB A \AS ‚Üí \AB B} is as well.
The monoid operation {\AB{\_‚àô\_} \AS : \AF{Op‚ÇÇ} \AB{B}} lifts to the function-level binary operation {\AS Œª (\AB{f} \AB{g} : \AB{A} \AS ‚Üí \AB{B}) \AS ‚Üí \AS Œª \AB a \AS ‚Üí \AB f \AB a \AB ‚àô \AB g \AB a}.
The monoid identity {\AB Œµ \AS : \AB{B}} lifts to the identity {\AS Œª \AB a \AS ‚Üí \AB Œµ}.
All of the laws transfer from \AB{B} to {\AB A \AS ‚Üí \AB B}.
Likewise for other algebraic structures.

Looking more closely, additional algebraic structure emerges on predicates: (full) \emph{semimodules} generalize vector spaces by relaxing the associated types of ``scalars'' from fields to commutative semirings, i.e., dropping the requirements of multiplicative and additive inverses \needcite{}.
Left and right semimodules further generalize scalars to arbitrary semirings, dropping the commutativity requirement for scalar multiplication.
Again, nothing is needed from the codomain type \AB{B} beyond the properties of a semiring, so again predicates get these algebraic structures for free (already paid for by types).
For every {\AB{P} \AS : \AF{Pred} \AB{A}}, \AB{P} is a vector in the semimodule {\AF{Pred} \AB{A}}, including the special case of ``languages'', for which \AB{A} is a list type.
The dimension of the semimodule is the cardinality of the domain type \AB{A} (typically infinite), with ``basis vectors'' having the form {\AF{pure‚±Ω} \AB{a}} for {\AB{a} \AS : \AB{A}}.
A general vector (predicate) is a linear combination (often infinite) of these basis vectors, with addition being union and scaling conjoining membership proofs.

\begin{code}[hide]
‚ãÜ-‚à™-isClosedSemiring : IsClosedSemiring _‚ü∑_ _‚à™_ _‚ãÜ_ ‚àÖ ùüè  _‚òÜ
‚ãÜ-‚à™-isClosedSemiring = record { isSemiring = ‚ãÜ-‚à™-isSemiring ; starÀ° = ‚òÜ-starÀ° }

‚ãÜ-‚à™-isClosedCommutativeSemiring :
  Commutative _‚â°_ _‚àô_ ‚Üí IsClosedCommutativeSemiring _‚ü∑_ _‚à™_ _‚ãÜ_ ‚àÖ ùüè _‚òÜ
‚ãÜ-‚à™-isClosedCommutativeSemiring ‚àô-comm = record
  { isCommutativeSemiring = ‚ãÜ-‚à™-isCommutativeSemiring ‚àô-comm
  ; starÀ° = ‚òÜ-starÀ°
  }

‚ãÜ-‚à™-ClosedSemiring : ClosedSemiring _ _
‚ãÜ-‚à™-ClosedSemiring = record { isClosedSemiring =  ‚ãÜ-‚à™-isClosedSemiring }

‚ãÜ-‚à™-ClosedCommutativeSemiring : Commutative _‚â°_ _‚àô_ ‚Üí ClosedCommutativeSemiring _ _
‚ãÜ-‚à™-ClosedCommutativeSemiring ‚àô-comm =
  record { isClosedCommutativeSemiring = ‚ãÜ-‚à™-isClosedCommutativeSemiring ‚àô-comm }
\end{code}

There is still more algebraic structure to be found and exploited.
When our \AB{domain} type \AB{A} is a monoid (as with languages, infinite streams and grids, and functions of continuous space and time), predicates over \AB{A} form a second semiring, known as ``the monoid semiring'' \needcite{}, in which zero and addition are as in the first predicate commutative semiring and semimodule, while one and multiplication are \AF{ùüè} and \AF{\_‚ãÜ\_}, defined in \secref{Languages, Predicates, and Types}.
In this setting, language concatenation is subsumed by a very general form of \emph{convolution} \needcite{}.
The semimodule structure of predicates provides the additive aspect and is built entirely from the \emph{codomain}-transforming predicate operations defined in \secref{Languages, Predicates, and Types}.
The multiplicative aspect (convolution) comes entirely from the \emph{domain}-transforming operations, applied to any domain monoid.
The two needed distributive properties hold for \emph{any} domain-transforming operation, whether associative or not.
The proofs of these properties, while not terribly complex, are beyond the scope of this paper but are available in the paper's Agda source code.\notefoot{There will be short and long forms of this paper. Perhaps the long form will contain narrated parts of the monoid semiring proof.}

Not only do types form a closed semiring, the \emph{only} properties needed from types in the monoid semiring come from the laws of closed semirings.
We can thus apply the ideas in this paper to many other useful semirings as well, including natural numbers (e.g., number of parses), probability distributions, and the tropical semirings (max/plus and min/plus) for optimization.
\out{We will explore this generality in \secref{Beyond Predicates}.}
As a bonus, when the domain monoid commutes (multiplicatively)---e.g., for functions of space and time rather than languages---the monoid semiring does as well .


\subsectionl{Transferring properties}

\end{comment}


\bibliography{bib}

\end{document}
