\newif\iftalk
\newif\ifacm

\acmtrue

\ifacm
\documentclass[acmsmall,screen]{acmart}
\else
\documentclass[acmsmall,screen,timestamp,nonacm]{acmart}
\fi

\usepackage{catchfilebetweentags}

\RequirePackage{newunicodechar, stmaryrd, setspace, comment, scalerel, doi}

\input{commands}
\input{unicode}
\input{macros}

%% Use oadoi.org instead of doi.org. Doesn't seem to take.
\makeatletter
\def\@formatdoi#1{\url{https://oadoi.org/#1}}
\makeatother

\definecolor{mylinkcolor}{rgb}{0,0.2,0}
\hypersetup{
  linkcolor  = mylinkcolor,
  citecolor  = mylinkcolor,
  urlcolor   = mylinkcolor,
  colorlinks = true, % false
}

\bibliographystyle{ACM-Reference-Format}
\citestyle{acmauthoryear}
\setcitestyle{nosort}

%% I prefer âˆ… in XITSMath-Regular, but I don't know how to make setmathfont
%% work with acmart.
\newunicodechar{â—‡}{\raisebox{0.02ex}{\ensuremath{_{\diamond}\hspace{-0.323em}}}}
\newunicodechar{â–¡}{\raisebox{-0.12ex}{\ensuremath{\scaleobj{0.4}{\Box}}\hspace{-0.238em}}}

\usepackage{libertine}  %% [tt=false]
\usepackage{agda}% references

%% I think acmart minimizes these two.

%% Switching to \textsf for AgdaFunction messes up vertical alignment.
%% \newcommand{\AgdaFontStyle}[1]{\textsf{#1}}

%% \renewcommand{\AgdaFontStyle}[1]{\text{#1}}

%% \renewcommand{\AgdaFunction}[1]
%%     {\AgdaNoSpaceMath{\text{\textcolor{AgdaFunction}{\AgdaFormat{#1}{\AgdaLink{#1}}}}}}
%% \renewcommand{\AgdaRecord}[1]
%%     {\AgdaNoSpaceMath{\text{\textcolor{AgdaRecord}{\AgdaFormat{#1}{\AgdaLink{#1}}}}}}

%% \rnc\AgdaTarget[1]{}

\author{Conal Elliott}
\affiliation[obeypunctuation=true]{%
%% \institution{Independent}
\country{USA}}
\email{conal@conal.net}

%% \setcopyright{rightsretained}
%% \acmJournal{PACMPL}
%% \acmYear{2021} \acmVolume{5} \acmNumber{ICFP} \acmArticle{78} \acmMonth{8} \acmPrice{}\acmDOI{10.1145/3473583}

%%% The following is specific to ICFP '21 and the paper
%%% 'Symbolic and Automatic Differentiation of Languages'
%%% by Conal Elliott.
%%%
\setcopyright{rightsretained}
\acmPrice{}
\acmDOI{10.1145/3473583}
\acmYear{2021}
\copyrightyear{2021}
\acmSubmissionID{icfp21main-p87-p}
\acmJournal{PACMPL}
\acmVolume{5}
\acmNumber{ICFP}
\acmArticle{78}
\acmMonth{8}

\begin{document}

\setlength{\abovedisplayskip}{1.5ex}
\setlength{\belowdisplayskip}{1.5ex}
\setlength{\abovedisplayshortskip}{0ex}
\setlength{\belowdisplayshortskip}{0ex}

%% \nc\tit{Working Title}
\nc\tit{%Functional Pearl:
Symbolic and Automatic Differentiation of Languages}

\title{\tit}
\date{Version of \DTMnow{} GMT-8}

\begin{abstract}
Formal languages are usually defined in terms of set theory.
Choosing type theory instead gives us languages as type-level predicates over strings.
Applying a language to a string yields a type whose elements are language membership proofs describing \emph{how} a string parses in the language.
The usual building blocks of languages (including union, concatenation, and Kleene closure) have precise and compelling specifications uncomplicated by operational strategies and are easily generalized to a few general domain-transforming and codomain-transforming operations on predicates.

A simple characterization of languages (and indeed functions from lists to any type) captures the essential idea behind language ``differentiation'' as used for recognizing languages, leading to a collection of lemmas about type-level predicates.
These lemmas are the heart of two dual parsing implementations---using (inductive) regular expressions and (coinductive) tries---each containing the same code but in dual arrangements (with representation and primitive operations trading places).
The regular expression version corresponds to symbolic differentiation, while the trie version corresponds to automatic differentiation.

The relatively easy-to-prove properties of type-level languages transfer almost effortlessly to the decidable implementations.
In particular, despite the inductive and coinductive nature of regular expressions and tries respectively, we need neither inductive nor coinductive/bisimulation arguments to prove algebraic properties.
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10003752.10003790.10011740</concept_id>
<concept_desc>Theory of computation~Type theory</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003752.10003766.10003776</concept_id>
<concept_desc>Theory of computation~Regular languages</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003752.10010124.10010138.10010142</concept_id>
<concept_desc>Theory of computation~Program verification</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003752.10010124.10010138.10010145</concept_id>
<concept_desc>Theory of computation~Parsing</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Theory of computation~Type theory}
\ccsdesc[500]{Theory of computation~Regular languages}
\ccsdesc[500]{Theory of computation~Program verification}
\ccsdesc[300]{Theory of computation~Parsing}

\keywords{program calculation; language derivatives; tries}

\maketitle
\mathindent2em

\sectionl{Specifying Languages}
\rnc\source{Language}

Languages are usually formalized either set-theoretically as sets of strings or operationally as parsers.
Alternatively, one can use type theory, so that a language is a type-level predicate on ``strings'' (lists of an arbitrary type \AB{A} of ``characters'').
The usual language operations are defined in \figrefdef{Lang-ops}{Languages as type-level predicates}{\agda{Lang-ops}}, including language union and intersection ({\AB{P} \AF âˆª \AB{Q}} and {\AB{P} \AF âˆ© \AB{Q}}) with their identities (the empty language \AF âˆ… and universal language \AF ð’°), language concatenation ({\AB{P} \AF â‹† \AB{Q}}) and its identity (\AF ðŸ, containing only the empty string), single-character languages ({\AF ` \AB{c}}), Kleene star ({\AB{P} \AF â˜†}), and ``scalar multiplication'' (\mbox{\AB{s} \AF Â· \AB{P}}, which will prove useful later).

The definitions in \figref{Lang-ops} reflect the logical interpretation of types under the Curry-Howard (``propositions as types'') isomorphism, in which types represent propositions and elements (values) of a type represent proofs of the corresponding proposition \citep{Wadler2015propositions}.
The specific embodiment of Curry-Howard in \figref{Lang-ops} and throughout this paper is the language Agda \citep{Norell2008AFP, BoveEtAl2009Agda}, which is both programming language and proof assistant founded on Martin-LÃ¶f's intuitionistic type theory \citep{MartinLÃ¶f1984intuitionistic}.
Specifically,
\begin{itemize}

\item Types have type {\APT{Set} \AB â„“} for some universe level \AB â„“.
These levels avoid logical inconsistencies\out{ resulting from {\APT{Set} \AS : \APT{Set}}} but can be safely ignored in this paper (which is parametrized over \AB â„“).

\item The types \AD âŠ¥ and \ARe âŠ¤ represent falsity and truth respectively, with \AD âŠ¥ having no elements (i.e., uninhabited) and \ARe âŠ¤ having a single element \AFi{tt}.

\item For types (propositions) \AB A and \AB B, the types {\AB A \AD âŠŽ \AB B} and {\AB A \ARe Ã— \AB B} represent disjunction and conjunction respectively as non-dependent sum and products.

\item For a function {\AB F \AS : \AB A \AS â†’ \APT{Set} \AB{â„“}}, the types {\AF âˆƒ \AB F}  and {\AF âˆ€ \AB x \AS â†’ \AB F \AB x} represent existential and universal quantification as \emph{dependent} product and function types.

\item The type {\AB x \AD â‰¡ \AB y} represents \emph{propositional} equality (rather than computable equality) as a data type with the single constructor {\AIC{refl} \AS : \AS âˆ€ \AB x \AS â†’ \AB x \AD â‰¡ \AB x}.

\end{itemize}

In the definition of {\AB P \AF â˜†}, \AF{concat} flattens a list of lists into a single list via a right fold.
The predicate {\AD{All} \AB P} holds of a list \AB{ws} when \AB{P} holds for every element of \AB{ws}:\footnote{The data type constructors \AIC{[]} and \AIC{\_âˆ·\_} are overloaded here, referring to both lists (as indices) and \AD{All}.}\\
\agda{foldr-concat}
\vspace{-1ex}
\agda{All}
These three definitions are provided in the Agda standard library \citep{agda-stdlib}.

Note that for a language \AB{P} and string \AB{w}, {\AB{P} \AB{w}} is the type of \emph{proofs} that {\AB{w} \AF âˆˆ \AB{P}}, in other words \emph{explanations} of membership, or \emph{parsings}.
(If the type {\AB{P} \AB{w}} is uninhabited, then {\AB{w} âˆ‰ \AB{P}}.)
For instance, every proof of {\AB{w} \AF âˆˆ \AB{P} \AF âˆª \AB{Q}} contains a proof of {\AB{w} \AF âˆˆ \AB{P}} or of {\AB{w} \AF âˆˆ \AB{Q}} \emph{and} the knowledge of which one was chosen (with different proofs reflecting different choices).
Likewise, a proof of {w \AF âˆˆ \AB{P} \AF â‹† \AB{Q}}\out{ (language concatenation)} includes the choice of strings \AB{u} and \AB{v}, a proof that {\AB{w} \AD â‰¡ \AB{u} \AF âŠ™ \AB{v}}, and proofs that {\AB{u} \AF âˆˆ \AB{P}} and {\AB{v} \AF âˆˆ \AB{Q}}.
\rnc\source{Examples}
\figrefdef{language-examples}{Language examples\protect\footnotemark}{\agda{examples}} contains some examples of languages (left) and corresponding membership proofs (right).

%% I split the footnote mark & text and delayed the latter so the text would
%% go on the same page as the figure and stay on one page.

The use of type-level predicates makes for simple, direct specification, including the use of existential quantification (as in \figref{Lang-ops}).
As described in \secref{Properties}, it is also fairly easy to prove algebraic properties of predicates\out{, including the semimodule and semiring laws for both predicate semirings}.
These definitions do not give us decidable parsing, because general type inhabitation amounts to theorem proving.
As shown in the rest of this paper, however, type-level predicates serve as a simple, precise, and non-operational basis for specification and reasoning, with correct and computable parsing implementations following systematically as corollaries.

This paper makes the following specific contributions:
\begin{itemize}
\item
  A collection of lemmas are stated and proved about functions over lists, capturing the semantic essence of Brzozowski's syntactic technique of derivatives of regular expressions (\secref{Decomposing Languages}).
\item
  A duality is revealed between regular expressions and tries (prefix trees) in the context of language differentiation (\secref{From Languages to Parsing}).
  Each implementation contains exactly the same code but in transposed forms.
  Both are corollaries of the semantic lemmas and are automatically proved correct by type-checking.
\item
  The theory and algorithms are specified in terms of proof isomorphism rather than the usual, coarser relation of logical equivalence.
  As such, the lemmas for language concatenation and Kleene closure are more informative than in previous work, capturing \emph{all} parsings distinctly, including closure of languages containing the empty string.
\item
  The vocabulary of languages is generalized from predicates on lists to predicates on arbitrary types, and then re-specialized to the usual vocabulary (\secref{Predicate Algebra}).
  This generalization consists of two useful covariant applicative functors on functions (the usual one on functions-from-\AF{X} and another on functions-to-\APT{Set}).
  % Properties are proved in terms of this more general vocabulary.
\item
  Properties are stated first at the level of predicates, making them easy to understand and prove, thanks to elimination of operational details.
  These properties are then transported to the computable parsing implementations (\secref{Transporting Properties from Specification to Implementations}) with no additional proofs needed.
\end{itemize}
\footnotetext{Some explanatory comments:
\begin{itemize}

\item Agda variable names can contain most characters other than spaces and parentheses, e.g., ``\AB{aâˆªbâ˜†}''.

\item The alphabet type (\AB A in \figref{Lang-ops}) used for these examples is \AF{Char}, i.e., characters.

\item Single-item lists are notated with square brackets.

\item Variables defined but not used can be replaced by the anonymous pattern ``\AK{\_}'' (as in proofs on the right).

\item The \AF{injâ‚} and \AF{injâ‚‚} functions are left and right injections into a sum type, corresponding to the left and right introduction forms for logical disjunction.

%% \item The \AF{injâ‚} and \AF{injâ‚‚} functions are left and right injections/introductions into a sum/disjunction type.

\item Pairing\out{ (indicated by the infix comma operator)} is right-associative.
      Since the second and third examples are existential propositions, their inhabiting proofs are dependent pairs.
      Each comprises a division into substrings and a proof that the substrings concatenate to the given test string (the language's argument), along with language membership proofs for each substring.

\item Equality proofs in these examples are by normalization and hence given by \AIC{refl}.
\vspace{-2ex}% Avoid bogus blank footnote line spilling to next page
\end{itemize}}
Although many proofs are elided below, the source code for this paper is freely available and contains fully verified Agda code with no postulates \citep{Elliott2021-language-derivatives-repo}.
The type signatures and definitions displayed in the paper are extracted from this source code by the Agda compiler.


\sectionl{Decomposing Languages}
\rnc\source{Calculus}

In order to convert languages into parsers, it will help to understand how language membership relates to the algebraic structure of lists, specifically the monoid formed by \AF{\_âŠ™\_} and \AIC{[]}.
Generalizing from languages to functions from lists to \emph{any} type, define
\agda{Î½ð’Ÿ}
For languages, {\AF{ð’Ÿ} \AB{P} \AB{u}} is the set of \AB{u}-suffixes from \AB{P}, i.e., the strings \AB{v} such that {\AB{u} \AF âŠ™ \AB{v} \AF âˆˆ \AB{P}}.

Since {\AB{u} \AF âŠ™ \AIC{[]} \AD â‰¡ \AB{u}}, it follows that\footnote{Recall that Agda variable names can contain most characters other than spaces and parentheses, e.g., ``\AB{Î½âˆ˜ð’Ÿ}'' as declared here.}\footnote{For functions \AB{f} and \AB{g}, {\AB{f} \AF{â‰—} \AB{g}} is extensional equality, i.e., {\AS âˆ€ \AB x \AS â†’ \AB{f} \AB{x} \AD{â‰¡} \AB{g} \AB{x}}.}
\agda{Î½âˆ˜ð’Ÿ}
Moreover, the function \AF{ð’Ÿ} distributes over \AIC{[]} and \AF{\_âŠ™\_}:\footnote{In algebraic terms, argument-flipped \AF{ð’Ÿ} is a monoid homomorphism from lists to endofunctions.}
\agda{ð’Ÿ[]âŠ™}
These facts suggest that \AB{ð’Ÿ} can be computed one list element at a time via a more specialized operation, as is indeed the case:\footnote{Repeated \AF{Î´} application is expressed here via a standard left fold (with {\AB{X} \AS = \AB{A} \AF âœ¶ \AS â†’ \AB B} for \AF{ð’Ÿfoldl}):
\agda{foldl}
\vspace{-5ex}
}
\agda{Î´}
%\vspace{-4ex}
\agda{ð’Ÿfoldl}
Each use of \AF{Î´} thus takes us one step closer to reducing general language membership to nullability.
%% In other words,%
%% \agda{Î½âˆ˜foldlÎ´}
This simple observation is the semantic heart of the syntactic technique of \emph{derivatives of regular expressions} as used for efficient recognition of regular languages \citep{Brzozowski64}, later extended to parsing general context-free languages \citep{Might2010YaccID}.
Lemmas \AF{Î½âˆ˜ð’Ÿ} and \AF{ð’Ÿfoldl} liberate this technique from the assumption that languages are represented \emph{symbolically}, by some form of grammar (e.g., regular or context-free), as will be exploited in \secref{Automatic Differentiation}.

In terms of automata theory, derived languages are states, with \AB{P} being the initial state and {\AF{ð’Ÿ} \AB{P} \AB{u}} the state reached from \AB{P} after consuming the string \AB{u}; \AF{Î´} is the state transition function; and \AB{Î½} is the set of accepting states\out{ \needcite{}}.
Lemmas \AF{Î½âˆ˜ð’Ÿ} and \AF{ð’Ÿfoldl} capture the correctness of this state machine as a recognizer for the language \AB{P}.

Given the definitions of \AF{Î½} and \AF{Î´} above and of the language operations in \secref{Specifying Languages}, one can prove properties about how they relate, as shown in
\figrefdef{nu-delta-lemmas}{Properties of \AF{Î½} and \AF{Î´} for language operations}{\agda{Î½Î´-lemmas}}.
The correct-by-construction parsing algorithms in \secref{From Languages to Parsing} are corollaries of these properties.

There are three relations involved in \figref{nu-delta-lemmas}: propositional equality (``â‰¡''), (type) isomorphism (``â†”'') and extensional isomorphism (``âŸ·'').
Isomorphism relates types (propositions) whose inhabitants (proofs) are in one-to-one correspondence.
\emph{Extensional} (or ``pointwise'') isomorphism relates predicates isomorphic on every argument:
\ExecuteMetaData[Inverses.tex]{ext-iso}

The equalities in \figref{nu-delta-lemmas} are all proved automatically by normalization (i.e., their proofs are simply \AIC{refl}), while the other relations require a bit more work.
As with all lemmas in this paper shown with signatures only, full proofs are in the paper's source code and are formally verified by the Agda compiler (including during typesetting of this paper).


\sectionl{Decidability}
\rnc\source{Decidability}

For effective implementations, we must bridge the gap between a type (of membership proofs) and its decidable inhabitation.
Fortunately, there is a convenient and compositional way to do so.
For type \AB{A}, the type {\AD{Dec} \AB{A}} contains proof of \AB{A} or a proof of {\AF Â¬ \AB{A}} (defined as usual to mean {\AB{A} \AS â†’ \AD{âŠ¥}}):\footnote{The Agda standard library contains a more efficient version of \AD{Dec} \stdlibCitep{Relation.Nullary}.}
\agda{Dec}
For compositionality, we will use a few operations that lift decidability of types to decidability of constructions on those types, as shown in \figrefdef{compositional-dec}{Compositional decidability\protect\footnotemark}{\agda{compositional-dec}}.
Moreover, decidability lifts naturally from types to predicates:
\agda{Decidable}

With these definitions, we can succinctly formulate the problem of decidable language recognition: \emph{given a language \AB{P}, construct a term of type {\AF{Decidable} \AB{P}}.}
To accomplish this transformation, we will look for decidable building blocks that mirror the predicate vocabulary defined in \secref{Specifying Languages}.

\footnotetext{%
A few explanatory remarks:
\begin{itemize}

\item {\AS Î» \AS{()}} is the unique function from \AD âŠ¥ to any type (here also {\AD âŠ¥}),  sometimes called ``\AF{absurd}'' or ``\AF{âŠ¥-elim}''.

\item Recall that \AF{injâ‚} and \AF{injâ‚‚} are left and right injections into a sum type.

\item \AF{projâ‚} and \AF{projâ‚‚} are left and right projections out of a product type.

\item For functions {\AB f \AS : \AB A \AS â†’ \AB C} and {\AB g \AS : \AB B \AS â†’ C}, the function {\AF [ \AB f \AF , \AB g \AF ] \AS : \AB A \AF âŠŽ \AB B \AS â†’ \AB C} maps {\AIC{injâ‚} \AB a} to {\AB f \AB a} and {\AIC{injâ‚‚} \AB b} to {\AB g \AB b}.
% In this use, {\AB C} is \AD âŠ¥, since we are proving a negation.

\item \AF{\_âœ¶â€½} reflects the fact that the empty list exists for every element type (even \AF âŠ¥).
\end{itemize}
\vspace{-2ex}
}

Type isomorphisms (such as those in \figref{nu-delta-lemmas}) play an important role in decidability, namely that isomorphic types or predicates are equivalently decidable:
\agda{isomorphisms}
One direction of the isomorphism proves \AB{B} from \AB{A}, while the other proves {\AF Â¬ B} from {\AF Â¬ A}.
In fact, logical \emph{equivalence} suffices for the results in this paper, but the stronger condition of isomorphism enables variations such as generating many parses/proofs rather than just one.

\sectionl{From Languages to Parsing}

\subsectionl{Reflections}

The lemmas in \figref{nu-delta-lemmas} tell us how to decompose languages defined in terms of the vocabulary from \figref{Lang-ops}, while the definitions in \figref{compositional-dec} tell us how compute inhabitation of the resulting types, resulting in decidable parsing.
These lemmas and definitions cannot be applied \emph{automatically} in their present form, however, because languages are functions, as are \AF{Î½} and \AF{Î´}, and so are not subject to structural inspection.
An automatic solution would need some form of \emph{reflection} of language-constructing operations or of \AF{Î½} and \AF{Î´} as inspectable data.

This situation is exactly as in differential calculus, since differentiation in that setting is also defined on functions rather than on symbolic representations.
Fortunately, there are two standard solutions for \emph{computable} differentiation, commonly referred to as ``symbolic'' and ``automatic'' differentiation.\out{\footnote{There is also an incorrect (approximate) variation often referred to as ``numeric'' differentiation.}}
In the former, functions are represented symbolically in some suitable vocabulary, and pattern-matching is used to apply rules of differentiation to those symbolic representations.
The latter solution involves re-interpreting the function-building vocabulary to construct not just the usual functions, but also their derivatives \citep{Griewank89onAD, GriewankWalther2008EvalDerivs, Elliott-2018-ad-icfp}.
This latter option is often much more efficient than the former, since it easily avoids a good deal of work duplicated between a function and its derivative.

\secreftwo{Symbolic Differentiation}{Automatic Differentiation} apply the symbolic and automatic strategies respectively to languages.
Both algorithms are corollaries of the lemmas in \figref{nu-delta-lemmas} and are automatically proved correct by type checking.

\subsectionl{Symbolic Differentiation}
\rnc\source{Symbolic}

\nc\api{\setstretch{1.1}\agda{api}}

The ``symbolic differentiation'' style reflects language-building vocabulary (\figref{Lang-ops}) into an inductive data type (of modestly extended regular expressions), while \AF{Î½} and \AF{Î´} become functions defined by pattern-matching on that data type.
For convenience, use the same names as in \secref{Specifying Languages} for these new counterparts, and refer to the original versions via the module prefix ``{â—‡.}\hspace{0.05em}''.
The result is shown in \figrefdef{symbolic-api}{Regular expressions (inductive)}{\api} along with a computable function {\AF{âŸ¦\_âŸ§â€½}} that converts a symbolic language representation to a computable parser.
Note that the \AF{\_â—‚\_} operation from \secref{Decidability} has been reified as a new constructor alongside the more conventional language operators.
The reason \AF{\_â—‚\_} must be part of the inductive representation is the same as the other constructors, namely so that the core lemmas (\figref{nu-delta-lemmas}) translate into an implementation in terms of that representation.

The \emph{meaning} of any {\AB r \AK{:} \AF{Lang} \AB P} is defined simply as \AB{P}, ignoring the representation \AB{r} itself\out{ (beyond its type)}:
\agda{semantics}

Decidable parsing relies only on \AF{Î½} and \AF{Î´} (via {\AF{âŸ¦\_âŸ§â€½}}), which are defined in \figrefdef{symbolic-defs}{Symbolic differentiation (column-major/patterns)}{\agda{defs}}, as systematically derived from the lemmas of \figref{nu-delta-lemmas}.\out{\notefoot{Describe this derivation.}}
These clauses are meant to be read in column-major order, i.e., each column is a complete definition (two definitions of ten pattern-matching clauses each).
Correctness is guaranteed by the types of \AF{Î½} and \AF{Î´} and so is proved automatically by type-checking \figref{symbolic-defs}.
(Recall from \secref{Decidability} that correctness is defined as constructing a term of type {\AF{Decidable} \AF{P}} for a given language \AB{P}, as satisfied by the type of {\AF{âŸ¦\_âŸ§â€½}}.)
We could thus use \emph{any} definitions of \AF{Î½} and \AF{Î´} that type- and termination-check, although not many definitions would do so.

In \figref{symbolic-defs}, note the role of the isomorphisms from \figref{nu-delta-lemmas}.
Since those isomorphisms relate \AF{Î½} and \AF{Î´} on the language-building operations to types and languages expressed in that \emph{same} vocabulary, we end up with a recursive algorithm.

\subsectionl{Automatic Differentiation}
\rnc\source{Automatic}

\agda{rules}
\rnc\rulesSep{-3ex}

\secref{Symbolic Differentiation} embodies one choice of reflecting functions into a data representation.
Now consider the dual strategy of ``automatic differentiation''.
This time, reflect \AF{Î½} and \AF{Î´} into a \emph{coinductive} data type of tries\footnote{The classic trie (``prefix tree'') data structure was introduced by \citet{Thue1912Gegenseitige} to represent sets of strings \citep[Section 6.3]{Knuth1998ACP3}, later generalized to arbitrary non-nested algebraic data types \citep{Connelly1995GenTrie}, and then from sets to functions \citep{Hinze2000GGT}.}, while redefining the language-building vocabulary as functions on that data type defined by \emph{copatterns} \citep{AbelEtAl12013, AbelPientka2016}.\footnote{Inductive types (``data'') describe finitely large values and are processed recursively (as least fixed points) by pattern-matching clauses that decompose arguments.
Dually, coinductive types (``codata'') describe infinitely large values and are processed corecursively (as greatest fixed points) by copattern-matching clauses that compose results.
Programs on inductive types are often proved by induction, while programs on coinductive types are often proved by coinduction\out{ and specifically bisumulation}.
(As we will see in \secref{Transporting Properties from Specification to Implementations}, however, the simple relationship to the specification in \secref{Decomposing Languages} allows many trivial correctness proofs, needing neither induction or coinduction.)
See \citet{Gordon1995:coinduction} for a tutorial on theory and techniques of coinduction.

Tries in general represent functions, with each trie datum position corresponding to a domain value.
Even when each domain value is finitely large, there are often infinitely many of them (e.g., lists), so tries will naturally be infinite and thus more amenable to coinductive than inductive analysis.}
(This program duality pattern has been noted and investigated by \citet{OstermannJabs2018}.)
Again, we will use the same names as in \secref{Specifying Languages}, referring to the original versions via the module prefix ``{â—‡.}\hspace{0.05em}''.
The result is shown in \figrefdef{automatic-api}{Tries (coinductive)}{\api}.

Decidable recognition again relies only on \AF{Î½} and \AF{Î´}, which are defined for each language operation in \figrefdef{automatic-defs}{Automatic differentiation (row-major/copatterns)}{\agda{defs}} (though with a problem to be noted and fixed in the next paragraph).
These clauses are meant to be read in row-major order, i.e., each row is a complete definition (ten definitions of two copattern-matching clauses each).
Note that the clauses in \figref{automatic-defs} are syntactically identical to those in \figref{symbolic-defs} but are organized dually.
(The compiler-generated syntax coloring differs to reflect the changed interpretation of the definitions.)

The proof of correctness (defined by {\AF{âŸ¦\_âŸ§}} and {\AF{âŸ¦\_âŸ§â€½}}, defined as before) is \emph{almost} accomplished by type- and termination-checking, but a technical problem arises.
The {\AF{Î´} (\AB{p} \AF â‹† \AB{q})} clause does not satisfy Agda's termination checker, which cannot see that the argument {\AF{Î´} \AB{p} \AB{a}} in the recursive use of \AF{\_â‹†\_} is in some sense smaller than \AB{p}.
(\figref{automatic-defs} compiles only due to suppressing termination checking for the problematic clause with a compiler pragma.)
Fortunately, this issue was already identified and solved by \citet{Abel2016}---also in the setting of trie-based language recognition---by using \emph{sized types} \citep{Abel2008, AbelPientka2016}.
\rnc\source{SizedAutomatic}
This solution only requires giving \AF{Lang} (now tries) an index {\AB{i} \AK : \APT{Size}} corresponding to the maximum depth to which a trie can be searched, or equivalently the longest string that can be matched\out{ (via {\AF{âŸ¦\_âŸ§â€½}})}.
In practice, we will work with arbitrarily deep tries, i.e., ones having index \APo{âˆž}, as in the type of {\AF{âŸ¦\_âŸ§â€½}}.

The modified representation is shown in \figrefdef{sized-automatic-api}{Sized tries (coinductive)}{\api}.
Decidable recognition is defined exactly as with unsized tries (\figref{automatic-defs}), with the sole exception of removing the compiler pragma that suppressed termination checking.
This time the compiler successfully proves \emph{total} correctness (including termination).

\vspace{3ex}

\rnc\source{Calculus}

To see the parallel with automatic differentiation (AD) more clearly, note that AD implementations sample a function \emph{and} its derivative together to exploit the fact that these two computations typically share much common work.
This work sharing also applies when differentiating languages.
Without this optimization we have
\agda{ð’Ÿâ€²}
\begin{samepage}
The potential work-sharing version exploits the close relationship between \AB{f} and {\AF{ð’Ÿ} \AB{f}}:
\agda{Ê»ð’Ÿ}
\end{samepage}
Equivalence follows from a simple inductive argument, thanks to lemmas \AF{Î½âˆ˜ð’Ÿ} and \AF{ð’Ÿfoldl} of \secref{Decomposing Languages}:
\agda{ð’Ÿâ€²â‰¡Ê»ð’Ÿ}
The trie representation exploits this sharing potential by weaving \AF{Î½} and \AF{Î´} into a single structure based on common prefixes.

\sectionl{Generalizing from languages to predicates}

\rnc\source{Predicate}

The language-building vocabulary defined in \secref{Specifying Languages} can be simplified and generalized.
First note that there are two categories of operations.
One category transforms the codomain (types): \AF{âˆ…}, \AF ð’°, \AF{\_âˆª\_}, and \AF{\_âˆ©\_}.
The other transforms the domain (lists): \AF{ðŸ}, \AF{\_â‹†\_}, \AF{\_â˜†}, and \AF{`}.
The first category applies to predicates over all types (not just over lists).
Within the second category, \AF{ðŸ}, \AF{\_â‹†\_}, and \AF{\_â˜†} can apply to any monoid, while \AF{`} is specific to lists.

\figrefdef{predicates}{Predicate operations}{
\small
\begin{center}
\mathindent-12ex
\agda{Pred}
\end{center}
\vspace{1.5ex}
\hfill
\mathindent0ex
\begin{minipage}[b]{19em}
\agda{codomain-transformers}
\end{minipage}
\hfill
\begin{minipage}[b]{19em}
\agda{domain-transformers}
\end{minipage}
\hfill\;
} shows two parallel collections of predicate operations, each covariantly transforming the codomain (left) or domain (right).\footnote{The types of operations on the left can be further relaxed from {\APT{Set} \AB â„“} to any codomain type, while keeping \AB{A} fixed, resulting in the usual covariant applicative functor of functions \emph{from} a fixed type \citep{McBride2008APE}.
Dually, the operations on the right (in their current generality) form a second covariant applicative functor of functions \emph{to} types.}
These generalized operations then specialize to language operations as in \figrefdef{lang-via-pred}{Languages via predicate operations}{
\mathindent0ex
\begin{center}
\mathindent-12ex
\agda{Lang}
\end{center}
\vspace{1ex}
\hfill
\begin{minipage}[c]{11em}
\setstretch{1.7}
\agda{codomain-ops}
\end{minipage}
\hfill
\begin{minipage}[c]{15em}
\setstretch{2.5}
\agda{domain-ops}
\end{minipage}
\hfill\;
}, which adds language complement (\AF âˆ).\footnote{The list-specific operations \AF{\_âŠ™\_}, \AIC{[]}, and \AF{concat} used in \figref{Lang-ops} have been generalized to monoid operations {\AF{\_âˆ™\_} and \AF{Îµ}}.
The \emph{definitions} of \AF{ðŸ}, \AF{\_â‹†\_}, and \AF{\_â˜†} do not require the monoid properties, but their properties will (\secref{Predicate Algebra}).
}

\rnc\source{Calculus}
The \AF{Î½} and \AF{Î´} lemmas in \figref{nu-delta-lemmas} for codomain (but not domain) operations are easily generalized as shown in \figrefdef{nu-delta-codomain-lemmas}{Properties of \AF{Î½} and \AF{Î´} for predicate codomain operations}{\setstretch{1.5}\agda{Î½Î´-codomain}}, all proved automatically by normalization.
Since the decidable language implementations in \secref{From Languages to Parsing} are corollaries of \AF{Î½} and \AF{Î´} lemmas, those implementations adapt easily to the generalized codomain operations (\AF{pureáµ€}, \AF{mapáµ€}, and \AF{mapáµ€â‚‚}), resulting in a somewhat smaller implementation and broader coverage.

\begin{samepage}
\sectionl{Properties}

\subsectionl{Predicate Algebra}
\end{samepage}

The basic building blocks of type-level predicates---and languages in particular---form the vocabulary of a \emph{closed semiring} in two different ways\out{, as reflected in the structure of regular expressions \needcite{}}.
The semiring abstraction has three aspects: (a) a commutative monoid providing ``zero'' and ``addition'', (b) a (possibly non-commutative) monoid providing ``one'' and ``multiplication'', and (c) the relationship between them, namely that multiplication distributes over addition and zero.\footnote{Distribution of multiplication over zero is also known as ``annihilation''.}
In the first predicate semiring, which is \emph{commutative} (i.e., multiplication commutes), zero and addition are \AF âˆ… and \AF{\_âˆª\_}, while one and multiplication are \AF ð’° and \AF{\_âˆ©\_}.
Closure adds a star/closure operation \AF{\_âœ¯} with \AF{star} law: {\AB x \AF âœ¯ \AF â‰ˆ 1 \AF + \AB x \AF âœ² \AB x \AF âœ¯}.

Conveniently, booleans and types form commutative semirings with all necessary proofs already in Agda's standard library.\footnote{The equivalence relation used for types is isomorphism rather than equality.
The boolean semiring is idempotent, while the type semiring is not (in order to distinguish multiple parsings in ambiguous languages).
The latter non-idempotence accounts for the difference between the \AF Î½ and \AF Î´ lemmas for {\AB P â‹† \AB Q} and {\AB P â˜†} in \figref{nu-delta-lemmas} and the corresponding rules in previous work, as mentioned in \secref{Related Work}.}
They are both closed as well.
For booleans, closure maps both \AIC{false} and \AIC{true} to \AIC{true}, with the \AF{star} law holding definitionally.
For types, the closure of \AB{A} is {\AB{A} âœ¶} (the usual inductive list type) with a simple, non-inductive proof of \AF{star}.

This first closed semiring for predicates follows from a much more general pattern.
Given any two types \AB{A} and \AB{B}, if \AB{B} is a monoid then {\AB A \AS â†’ \AB B} is as well.
The monoid operation \AB{\_âˆ™\_} lifts to the function-level binary operation {\AS Î» (\AB{f} \AB{g} : \AB{A} \AS â†’ \AB{B}) \AS â†’ \AS Î» \AB a \AS â†’ \AB f \AB a \AB âˆ™ \AB g \AB a}.
The monoid identity {\AB Îµ \AS : \AB{B}} lifts to the identity {\AS Î» \AB a \AS â†’ \AB Îµ}.
All of the laws transfer from \AB{B} to {\AB A \AS â†’ \AB B}.
Likewise for other algebraic structures.
(As always, compiler-verified proofs are in this paper's source code.)

Looking more closely, additional algebraic structure emerges on predicates: (full) \emph{semimodules} generalize vector spaces by relaxing the associated types of ``scalars'' from fields to commutative semirings, i.e., dropping the requirements of multiplicative and additive inverses.
Left and right semimodules further generalize scalars to arbitrary semirings, dropping the commutativity requirement for scalar multiplication.
Again, nothing is needed from the codomain type \AB{B} beyond the properties of a semiring, so again predicates get these algebraic structures for free (already paid for by types).
Every {\AB{P} \AS : \AF{Pred} \AB{A}} is a vector in the semimodule {\AF{Pred} \AB{A}}, including the special case of ``languages'', for which \AB{A} is a list type.
The dimension of the semimodule is the cardinality of the domain type \AB{A} (typically infinite), with ``basis vectors'' having the form {\AF{pureâ±½} \AB{a}} for {\AB{a} \AS : \AB{A}}.
A general vector (predicate) is a linear combination (often infinite) of these basis vectors, with addition being union and scaling defined by pairing membership proofs (as in \figref{lang-via-pred}).

There is still more algebraic structure to be found and exploited.
When our \AB{domain} type \AB{A} is a monoid (as with languages, infinite streams and grids, and functions of continuous space and time), predicates over \AB{A} form a second semiring, known as ``the monoid semiring'' \citep{Golan2005RecentSemi}, in which zero and addition are as in the first predicate commutative semiring and semimodule, while one and multiplication are \AF{ðŸ} and \AF{\_â‹†\_}, defined in \figreftwo{Lang-ops}{lang-via-pred}.
In this setting, language concatenation is subsumed by a very general form of \emph{convolution} \cite{Golan2005RecentSemi, Dongol2016CUC, Elliott2019-convolution}.
The semimodule structure of predicates provides the additive aspect and is built entirely from the \emph{codomain}-transforming predicate operations defined in \figref{Lang-ops} and redefined in \figref{lang-via-pred}.
The multiplicative aspect (convolution) comes entirely from the \emph{domain}-transforming operations applied to any domain monoid.
The two needed distributive properties hold for \emph{any} domain-transforming operation, whether associative or not.
The formal statements and proofs of these properties are included in this paper's Agda source code \citep{Elliott2021-language-derivatives-repo}.

Not only do types form a closed semiring, the \emph{only} properties needed from types in the monoid semiring come from the laws of closed semirings.
One can thus apply the ideas in this paper to many other useful semirings, including natural numbers (e.g., number of parses), probability distributions, and the tropical semirings (max/plus and min/plus) for optimization.
\out{We will explore this generality in \secref{Beyond Predicates}.}
When the domain monoid commutes (multiplicatively)---e.g., for functions of space and time\out{ rather than languages}---the monoid semiring does as well.
Applications include power series (univariate and multivariate) and image processing \citep{Elliott2019-convolution}.
Research on semirings in parsing began with \citet{Chomsky1959CFL} and was further explored by \citet{Goodman1998PIO, Goodman1999SP} and by \citet{Liu2004}.

\vspace{2.5ex}
\subsectionl{Transporting Properties from Specification to Implementations}

\rnc\source{Transport}

We have seen that type-level languages have useful and illuminating algebraic properties and that the decidable implementations in \secref{From Languages to Parsing} are tightly connected to languages.
Must we prove these properties again for each implementation---where there are more operational details to manage---or do the algebraic properties somehow transfer to those implementations?
This question immediately raises a technical obstacle.
Algebraic abstractions and their properties are simply typed.
For example, each instance of the \ARe{Monoid} abstraction involves a single type \AB Ï„ with a binary operation {\AB{\_âˆ™\_} \AK : \AB Ï„ \AK â†’ \AB Ï„ \AK â†’ \AB Ï„} and identity value {\AB Îµ \AK : \AB Ï„}.
This recipe accommodates the language operations in \figreftwo{Lang-ops}{lang-via-pred} and their generalizations in \figref{predicates}, but not the dependently typed, \emph{indexed} versions of languages and their operations in \figreftwo{symbolic-api}{sized-automatic-api}, nor decidable types and their vocabulary in \figref{compositional-dec}.

Fortunately, there is a simple way to encapsulate indexed types into non-indexed types, namely existential quantification.
For instance, {\AF{âˆƒ} \AD{Dec}} and {\AF{âˆƒ} \AD{(\ARe{Lang} \APo âˆž)}} are non-indexed types.
For a type family {\AB F \AK : \AB A \AK â†’ \APT{Set} \AB â„“}, values of type {\AF âˆƒ \AB F} are pairs {(\AB a , \AB b)} with {\AB a \AK : \AB A} and {\AB b \AK : \AB F \AB a}.
We can also encapsulate dependently typed \emph{operations} on indexed types into simply typed operations on the (simply typed) existentially wrapped versions of those types.
For instance, consider language union from \figref{symbolic-api}:
\begin{code}
% Copied from latex/Symbolic.tex
\>[2]\AgdaOperator{\AgdaInductiveConstructor{\AgdaUnderscore{}âˆª\AgdaUnderscore{}}}%
\>[7]\AgdaSymbol{:}\AgdaSpace{}%
\AgdaDatatype{Lang}%
\>[15]\AgdaGeneralizable{P}%
\>[18]\AgdaSymbol{â†’}\AgdaSpace{}%
\AgdaDatatype{Lang}\AgdaSpace{}%
\AgdaGeneralizable{Q}%
\>[28]\AgdaSymbol{â†’}\AgdaSpace{}%
\AgdaDatatype{Lang}\AgdaSpace{}%
\AgdaSymbol{(}\AgdaGeneralizable{P}%
\>[39]\AgdaOperator{\AgdaFunction{â—‡.âˆª}}%
\>[44]\AgdaGeneralizable{Q}\AgdaSymbol{)}\<%
\end{code}
Referring to the type and operations of \figreftwo{Lang-ops}{symbolic-api} (for the inductive language representation) via the module prefixes ``{â—‡.}\hspace{0.05em}'' and ``{â–¡.}\hspace{0.05em}'' respectively, define \agda{wrapped-Lang}
Then wrap union as follows:
\agda{wrapped-union}
Likewise for the coinductive language representation in \figref{sized-automatic-api}.
The type-level language components of the arguments and result are given explicitly here but could instead be inferred by the compiler.
\rnc\source{Existential}
Indexed operations of all arities\out{ for a type family \AB{F}} can be systematically wrapped in this style, as shown in
\figrefdef{inj}{Existential wrappers}{\mathindent0ex\agda{inj}}, again with automatically inferable components given explicitly for clarity.
Wrapped union can then be defined simply as {\AF{\_âˆª\_} \AS = \AF{injâ‚‚} \AIC{â–¡.\_âˆª\_}} and likewise for the other operations.

In addition to transporting types and operations as just described, we also need to transport \emph{properties}.
A crucial question is the choice of equivalence relation for the new properties.
If we choose equivalence on these dependent pairs to be \emph{semantic}---i.e., equivalence of \emph{first} projections (type \emph{indices})---then all algebraic properties of those indices will hold for the existentially wrapped versions as well.
This choice may at first seem like cheating, since the entire operational aspect of the representation is ignored.
The correctness of that aspect, however, is guaranteed by its dependent typing, which anchors its meaning (no matter how cleverly implemented) to the non-operational type index.
For the language implementations of \secref{From Languages to Parsing}, this anchoring is guaranteed by the types of \AF{âŸ¦\_âŸ§} and \AF{âŸ¦\_âŸ§â€½} in \figreftwo{symbolic-api}{sized-automatic-api}.
(Similarly, for the decidable type constructors in \figref{compositional-dec}, correctness is guaranteed by the types of the \AIC{yes} and \AIC{no} constructors in the definition of â€Œ\ARe{Dec} in \secref{Decidability}.)
Given this choice of equivalence, properties of index types easily lift to properties of existential types, as shown in \figrefdef{prop}{Liftings of index properties to existential types}{\mathindent0ex\agda{prop}}.
(These definitions are partially inferable as well, e.g., from \agda{propâ‚ƒâ€²}.)
These definitions ease specification of algebraic instances for existentially wrapped types from corresponding instances for their indices.
\rnc\source{Transport}
As examples, we can lift the commutative semiring of types to wrappings of the decidable counterparts from \figref{compositional-dec} in \secref{Decidability}, and lift both language semirings to both decidable implementations in \secref{From Languages to Parsing}, as in \figrefdef{transport-examples}{Examples of algebraic instances transported from indices to existential wrappings}{\agda{examples}}.


\sectionl{Related Work}

The shift from languages as sets of strings to type-level predicates (i.e., proof relevance or ``parsing'') is akin to weighted automata \citep{Schutzenberger1961, DrosteKuske2019} and more generally to semiring-based parsing \citep{Chomsky1959CFL, Goodman1998PIO, Goodman1999SP, Liu2004}, noting that types (``\APT{Set}'' in Agda) form a (commutative) semiring (up to isomorphism).
In particular, \citet{Lombardy2005} investigated Brzozowski-style derivatives in this more general setting, formalizing and generalizing the work of \citet{Antimirov1996}, who decomposed derivatives into simpler components (``partial derivatives'') that sum to the full derivative and lead to efficient recognizers in the form of nondeterministic automata.

Most work on formal languages in functional programming has been in parsing, particularly via parsing combinators \citep{Hutton1996monadic, Leijen2001parsec, Swierstra2008combinator}.
Such work typically concentrates on usability, composability, and sometimes efficiency, rather than on simple semantic specification and verifiable correctness.

The main contributions of this paper are to formalization and mechanization of language recognition and parsing.
There has been some work using type theory to capture languages as type-level predicates much as in \secref{Specifying Languages}:
\begin{itemize}

\item \citet{AgularMannaa2009} also defined a non-indexed algebraic type of regular expressions.
They also \emph{defined} indexed types for \AF{Î½} and \AF{Î´} on regular expressions rather than defining the meanings of those operations in terms of languages and then proving properties of them.
They proved a consistency property about \AF{Î´}, but apparently not about \AF{Î½}, and they note their method's ``inability to prove the non-membership of some string in a given language''.

\item \citet{DoczkalEtAl2013} formalized regular languages constructively in Coq, also in terms of a type-level membership predicate.
They then proved several classic results about finite automata, including minimization and relationships to Nerode and Myhill partitions.

\item \citet{FirsovUustalu2013} formalized regular expressions in Agda with a corresponding inductive type of language membership proofs and related regular expressions to nondeterministic finite automata (NFAs) represented as boolean matrices.
They also defined operations on that representation corresponding to those on regular expressions and proved soundness and completeness of the NFA representation with respect to regular language membership.

\item \citet{KorkutEtAl2016} defined a non-indexed algebraic data type of regular expressions together with an indexed inductive type of proofs of membership of strings in the language denoted by the regular expressions and a corresponding function that matches an implicitly concatenated stack of regular expressions.
Special attention was paid to ensuring and proving termination, particularly in regard to languages that contain the empty string.
Addressing both involved defunctionalization and translating between general regular expressions and a restricted ``standard'' form.

\item \citet{Traytel2017} defined formal languages as tries, with operations via primitive corecursion and reasoning via coinduction, all in the Isabelle/HOL proof assistant.
There appears to have been no specification higher level (less operational) than tries, leading to some rather complex definitions (which, as \emph{definitions}, cannot be proved correct).
Language concatenation was particularly delicate.
For the same reason, proofs of even basic properties involved coinduction.
The authors pointed out, however, that the coinductive trie formulation enabled better proof automation than with sets of strings.

\item \citet{Abel2016} also formulated languages via tries and in particular noted the termination issue mentioned in \secref{Automatic Differentiation} and provided the sized-types solution adopted in this paper.
Relationships to automata theory and coalgebraic notions were also explored in depth.
Trie look-up yielded boolean values rather than propositions, and there was no simpler specification of languages with respect to which the trie implementation could be specified and proved consistent, so every operation was \emph{defined} coinductively.
The concatenation and closure (star) cases were especially delicate.
Equality on tries was defined by strong bisimilarity, and thus properties required corresponding machinery to state and prove.

%% requiring more careful construction and reading than the specification given in \figref{Lang-ops} and used in the implementations in \secref{From Languages to Parsing}

\item \citet{BaanenSwierstra2020} also explored correctness of regular expression matching in Agda, but from the perspective of \emph{effects} and their predicate transformer semantics.
The authors shared a common starting point with \citet{KorkutEtAl2016} and addressed differentiation on regular expressions, while separating termination from partial correctness.
Their regular expression matching function targets a free monad (later to be interpreted with suitable effect semantics) rather than propositions.

\end{itemize}

Of the previous work involving language derivatives, none appear to have based their formal specification and proofs on the essential, non-inductive, nearly-trivial definition in terms of functions of lists from \secref{Decomposing Languages}.
\citet{Abel2016} mentioned this definition informally as motivation for the more complex, coinductively defined operations on tries.
\citet{Brzozowski64} also made clear mention of the underlying meaning on sets of strings in his original work on regular expression derivatives.
Moreover, none of these previous investigations appear to have addressed proof isomorphism (distinguishing multiple parsings/explanations), and correspondingly all use transformations based on the less precise laws originally discovered by \citet{Brzozowski64}.
Those laws were based on the assumption that union (``addition'' in both language semirings of \secref{Predicate Algebra}) is idempotent, which is not true for type-level language predicates related by proof isomorphism (rather than mere logical equivalence).
The lack of idempotence is crucial for going beyond recognizers to parsers (which are essentially \emph{proof-relevant} recognizers), in which we might want to extract more than one proof (parsing).
The difference is mainly visible in the \AF Î½ and \AF Î´ lemmas for {\AB P â‹† \AB Q} and {\AB P â˜†} in \figref{nu-delta-lemmas}.
As mentioned at the end of \secref{Decidability}, one can simplify the development above somewhat by replacing proof isomorphism with the coarser notion of logical equivalence.
Doing so restores union idempotence (modulo equivalence), losing proof relevance and thus replacing parsing by recognition.
One might then also prove that the set of derivatives of a regular language is finite (again, modulo equivalence).

%% The publisher wants URLs in the url field, while I prefer them in the title.
\ifacm
\bibliography{acm-bib}
\else
\bibliography{bib}
\fi

\end{document}
