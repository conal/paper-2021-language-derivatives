\newif\ifacm

\acmtrue

\ifacm
\documentclass[acmsmall,screen,anonymous,timestamp,review]{acmart}
%% \documentclass[acmsmall,screen,timestamp,nonacm]{acmart}
%% \authorsaddresses{}
\else
\documentclass[hidelinks]{article}  % fleqn,12pt
\usepackage[margin=1.3in]{geometry}  % 0.12in, 0.9in, 1in
\usepackage[useregional]{datetime2}
\fi

\usepackage{catchfilebetweentags}

%% \usepackage{balance}  % even out final two-column page

\ifacm\else
%% Already in acmart
\RequirePackage{amssymb, unicode-math}
\fi
\RequirePackage{newunicodechar, stmaryrd, setspace, comment, scalerel}

\input{commands}
\input{unicode}
\input{macros}

\definecolor{mylinkcolor}{rgb}{0,0.2,0}
\hypersetup{
  linkcolor  = mylinkcolor,
  citecolor  = mylinkcolor,
  urlcolor   = mylinkcolor,
  colorlinks = true, % false
}

\ifacm
\bibliographystyle{ACM-Reference-Format}
\citestyle{acmauthoryear}
\else
\usepackage[round]{natbib} % [square]
\bibliographystyle{plainnat}
\fi

%% I prefer âˆ… in XITSMath-Regular, but I don't know how to make setmathfont
%% work with acmart.
\ifacm
\newunicodechar{â—‡}{\raisebox{0.02ex}{\ensuremath{_{\diamond}\hspace{-0.323em}}}}
\newunicodechar{â–¡}{\raisebox{-0.12ex}{\ensuremath{\scaleobj{0.4}{\Box}}\hspace{-0.238em}}}
\else
\setmathfont{XITSMath-Regular.otf}
\newunicodechar{â—‡}{\raisebox{0.23ex}{\ensuremath{_{\diamond}\hspace{-0.358em}}}}
\newunicodechar{â–¡}{\raisebox{-0.13ex}{\ensuremath{\scaleobj{0.3}{\Box}}\hspace{-0.23em}}}
\newunicodechar{âˆ·}{\ensuremath{\mathbin{\hspace{-0.2em}:\hspace{-0.27em}:\hspace{-0.15em}}}}
\fi

\usepackage{libertine}  %% [tt=false]
\usepackage{agda}% references

%% Switching to \textsf for AgdaFunction messes up vertical alignment.
%% \newcommand{\AgdaFontStyle}[1]{\textsf{#1}}

%% \renewcommand{\AgdaFontStyle}[1]{\text{#1}}

%% \renewcommand{\AgdaFunction}[1]
%%     {\AgdaNoSpaceMath{\text{\textcolor{AgdaFunction}{\AgdaFormat{#1}{\AgdaLink{#1}}}}}}
%% \renewcommand{\AgdaRecord}[1]
%%     {\AgdaNoSpaceMath{\text{\textcolor{AgdaRecord}{\AgdaFormat{#1}{\AgdaLink{#1}}}}}}

%% \rnc\AgdaTarget[1]{}

\author{Conal Elliott}

\begin{document}

%% \nc\tit{Working Title}
\nc\tit{%Functional Pearl:
Symbolic and Automatic Differentiation of Languages}

\title{\tit}
\date{Version of \DTMnow{} GMT-8}

\ifacm\else
\maketitle
\fi

\begin{abstract}
Formal languages are usually defined in terms of set theory.
Choosing type theory instead gives us languages as type-level predicates over strings.
Applying a language to a string yields a type whose elements are language membership proofs describing \emph{how} a string parses in the language.
The usual building blocks of languages (including union, concatenation, and Kleene closure) have precise and compelling specifications uncomplicated by operational strategies and are easily generalized to a few general domain-transforming and codomain-transforming operations on predicates.

A simple characterization of languages (and indeed functions from lists to any type) captures the essential idea behind language ``differentiation'' (and integration) as used for recognizing languages, leading to a collection of lemmas about type-level predicates.
These lemmas are the heart of two dual parsing implementations---using (inductive) regular expressions and (coinductive) tries---each containing the same code but in dual arrangements (with representation and primitive operations trading places).
The regular expression version corresponds to symbolic differentiation, while the trie version corresponds to automatic differentiation.

The relatively easy-to-prove properties of type-level languages transfer almost effortlessly to the decidable implementations.
In particular, despite the inductive and coinductive nature of regular expressions and tries respectively, we need neither inductive nor coinductive/bisimulation arguments to prove algebraic properties.
\end{abstract}

\ifacm
\maketitle
\mathindent2em
\fi

\sectionl{Specifying Languages}
\rnc\source{Language}

Languages are usually formalized either set-theoretically as sets of strings or operationally as parsers.
Alternatively, one can use type theory, so that a language is a type-level predicate on ``strings'' (lists of an arbitrary type \AB{A} of ``characters'').
The usual language operations are defined in \figrefdef{Lang-ops}{Languages as type-level predicates}{\agda{Lang-ops}}, including language union and intersection ({\AB{P} \AF âˆª \AB{Q}} and {\AB{P} \AF âˆ© \AB{Q}}) with their identities (the empty language \AF âˆ… and universal language \AF ð’°), language concatenation ({\AB{P} \AF â‹† \AB{Q}}) and its identity (\AF ðŸ, containing only the empty string), single-character languages ({\AF ` \AB{c}}), Kleene star ({\AB{P} \AF â˜†}), and ``scalar multiplication'' (\mbox{\AB{s} \AF Â· \AB{P}}, which will prove useful later).

The definitions in \figref{Lang-ops} reflect the logical interpretation of types under the Curry-Howard (``propositions as types'') isomorphism, in which types represent propositions and elements (values) of a type represent proofs of the corresponding proposition \citep{Wadler2015propositions}.
The specific embodiment of Curry-Howard in \figref{Lang-ops} and throughout this paper is the language Agda \citep{Norell2008AFP, BoveEtAl2009Agda}, which is both programming language and proof assistant founded on Martin-LÃ¶f's intuitionistic type theory \citep{MartinLÃ¶f1984intuitionistic}.
Specifically,
\begin{itemize}

\item Types have type {\APT{Set} \AB â„“} for some universe level \AB â„“.
These levels avoid logical inconsistencies\out{ resulting from {\APT{Set} \AS : \APT{Set}}} but can be safely ignored in this paper (which is parametrized over \AB â„“).

\item The types \AD âŠ¥ and \ARe âŠ¤ represent falsity and truth respectively, with \AD âŠ¥ having no elements (i.e., uninhabited) and \ARe âŠ¤ having a single element \AFi{tt}.

\item The types {\AB A \AD âŠŽ \AB B} and {\AB A \ARe Ã— \AB B} represent disjunction and conjunction respectively as non-dependent sum and products.

\item For a function {\AB F \AS : \AB A \AS â†’ \APT{Set} \AB{â„“}}, the types {\AF âˆƒ \AB F}  and {\AF âˆ€ \AB x \AS â†’ \AB F \AB x} represent existential and universal quantification as \emph{dependent} product and function types.

\item The type {\AB x \AD â‰¡ \AB y} represents \emph{propositional} equality (rather than computable equality) as a data type with the single constructor {\AIC{refl} \AS : \AS âˆ€ \AB x \AS â†’ \AB x \AD â‰¡ \AB x}.

\end{itemize}

Note that for a language \AB{P} and string \AB{w}, {\AB{P} \AB{w}} is the type of \emph{proofs} that {\AB{w} \AF âˆˆ \AB{P}}, in other words \emph{explanations} of membership, or \emph{parsings}.
(If the type {\AB{P} \AB{w}} is uninhabited, then {\AB{w} âˆ‰ \AB{P}}.)
For instance, every proof of {\AB{w} \AF âˆˆ \AB{P} \AF âˆª \AB{Q}} contains a proof of {\AB{w} \AF âˆˆ \AB{P}} or of {\AB{w} \AF âˆˆ \AB{Q}} \emph{and} the knowledge of which one was chosen (with different proofs reflecting different choices).
Likewise, a proof of {w \AF âˆˆ \AB{P} \AF â‹† \AB{Q}}\out{ (language concatenation)} includes the choice of strings \AB{u} and \AB{v}, a proof that {\AB{u} \AF âŠ™ \AB{v} \AD â‰¡ \AB{w}}, and proofs that {\AB{u} \AF âˆˆ \AB{P}} and {\AB{v} \AF âˆˆ \AB{Q}}.

The use of type-level predicates makes for simple, direct specification, including the use of existential quantification.
As described in \secref{Properties}, it's also fairly easy to prove algebraic properties of predicates\out{, including the semimodule and semiring laws for both predicate semirings}.
These definitions do not give us decidable parsing, because general type inhabitation amounts to theorem proving.
As shown in the rest of this paper, however, type-level predicates serve as a simple, precise, and non-operational basis for specification and reasoning, with correct and computable parsing implementations following systematically as corollaries.

This paper makes the following specific contributions:
\begin{itemize}
\item
  A collection of lemmas are stated and proved about functions over lists, capturing the semantic essence of Brzozowski's syntactic technique of derivatives of regular expressions (\secref{Decomposing Languages}).
\item
  A duality is revealed between regular expressions and tries (prefix trees) in the context of language differentiation (\secref{From Languages to Parsing}).
  Each implementation contains exactly the same code but in transposed forms.
  Both are corollaries of the semantic lemmas and are automatically proved correct by type-checking.
\item
  The theory and algorithms are specified in terms of proof isomorphism rather than the usual, coarser relation of logical equivalence.
  As such, the lemmas for language concatenation and Kleene closure are more informative than in previous work, capturing \emph{all} parsings distinctly, including closure of languages containing the empty string.
\item
  The vocabulary of languages is generalized from predicates on lists to predicates on arbitrary types, and then re-specialized to the usual vocabulary (\secref{Predicate Algebra}).
  This generalization consists of two useful covariant applicative functors on functions (the usual one on functions-from-\AF{X} and another on functions-to-\APT{Set}).
  % Properties are proved in terms of this more general vocabulary.
\item
  Properties are stated first at the level of predicates, making them easy to understand and prove, thanks to elimination of operational details.
  These properties are then transported effortlessly to the computable parsing implementations (\secref{Transporting Properties}).
\end{itemize}
Although many proofs are elided below, the source code for this paper is freely available and contains fully verified Agda code with no postulates.
All type signatures and definitions in the paper are generated by the Agda compiler.


\sectionl{Decomposing Languages}
\rnc\source{Calculus}

In order to convert languages into parsers, it will help to understand how language membership relates to the algebraic structure of lists, specifically the monoid formed by \AIC{[]} and \AF{\_âŠ™\_}.
Generalizing from languages to functions from lists to \emph{any} type, define
\agda{Î½ð’Ÿ}
In terms of languages, {\AF{ð’Ÿ} \AB{P} \AB{u}} comprises all strings \AB{v} such that {\AB{u} \AF âŠ™ \AB{v} \AF âˆˆ \AB{P}}, i.e., the \AB{u}-suffixes of strings in \AB{P}.

Since {\AB{u} \AF âŠ™ \AIC{[]} \AD â‰¡ \AB{u}}, it follows that\footnote{For functions \AB{f} and \AB{g}, {\AB{f} \AF{â‰—} \AB{g}} is extensional equality, i.e., {\AS âˆ€ \AB x \AS â†’ \AB{f} \AB{x} \AD{â‰¡} \AB{g} \AB{x}}.}
\agda{Î½âˆ˜ð’Ÿ}
Moreover, the function \AF{ð’Ÿ} distributes over \AIC{[]} and \AF{\_âŠ™\_}:\footnote{In algebraic terms, argument-flipped \AF{ð’Ÿ} is a monoid homomorphism from lists to endofunctions.}
\agda{ð’Ÿ[]âŠ™}
These facts suggest that \AB{ð’Ÿ} can be computed one list element at a time via a more specialized operation, as is indeed the case:\footnote{Repeated \AF{Î´} application is expressed here via a standard left fold (with {\AB{X} \AS = \AB{A} \AF âœ¶ \AS â†’ \AB B} in \AF{ð’Ÿfoldl}):
\agda{foldl}
}
\agda{Î´}
%\vspace{-4ex}
\agda{ð’Ÿfoldl}
Each use of \AF{Î´} thus takes us one step closer to reducing general language membership to nullability.
%% In other words,%
%% \agda{Î½âˆ˜foldlÎ´}
This simple observation is the semantic heart of the syntactic technique of \emph{derivatives of regular expressions} as used for efficient recognition of regular languages \citep{Brzozowski64}, later extended to parsing general context-free languages \citep{Might2010YaccID}.
Lemmas \AF{Î½âˆ˜ð’Ÿ} and \AF{ð’Ÿfoldl} liberate this technique from the assumption that languages are represented \emph{symbolically}, by some form of grammar (e.g., regular or context-free), inviting other representations as we will see in \secref{Automatic Differentiation}.

In terms of automata theory, derived languages are states, with \AB{P} being the initial state and {\AF{ð’Ÿ} \AB{P} \AB{u}} the state reached from \AB{P} after consuming the string \AB{u}; \AF{Î´} is the state transition function; and \AB{Î½} is the set of accepting states\out{ \needcite{}}.
Lemmas \AF{Î½âˆ˜ð’Ÿ} and \AF{ð’Ÿfoldl} capture the correctness of this state machine as a recognizer for the language \AB{P}.

Given the definitions of \AF{Î½} and \AF{Î´} above and of the language operations in \secref{Specifying Languages}, one can prove properties about how they relate, as shown in
\figrefdef{nu-delta-lemmas}{Properties of \AF{Î½} and \AF{Î´} for language operations}{\agda{Î½Î´-lemmas}}.
The correct-by-construction parsing algorithms in \secref{From Languages to Parsing} are corollaries of these properties.

There are three relations involved in \figref{nu-delta-lemmas}: propositional equality (``â‰¡''), (type) isomorphism (``â†”'') and extensional isomorphism (``âŸ·'').
Isomorphism relates types (propositions) whose inhabitants (proofs) are in one-to-one correspondence.
\emph{Extensional} (or ``pointwise'') isomorphism relates predicates isomorphic on every argument:
\ExecuteMetaData[Inverses.tex]{ext-iso}

The equalities in \figref{nu-delta-lemmas} are all proved automatically by normalization (i.e., their proofs are simply \AIC{refl}), while the other relations require a bit more work.
As with all lemmas in this paper shown with signatures only, full proofs are in the paper's source code and are formally verified by the Agda compiler.


\sectionl{Decidability}
\rnc\source{Decidability}

For effective implementations, we must bridge the gap between a type (of membership proofs) and its decidable inhabitation.
Fortunately, there is a convenient and compositional way to do so.
For type \AB{A}, the type {\AF{Dec} \AB{A}} contains proof of \AB{A} or a proof of {\AF Â¬ \AB{A}} (defined as usual to mean {\AB{A} \AS â†’ \AD{âŠ¥}}):\footnote{This simple \AF{Dec} definition been superceded by a more efficient but more complex version \stdlibCitep{Relation.Nullary}.}
\agda{Dec}
For compositionality, we will use a few operations that lift decidability of types to decidability of constructions on those types, as shown in \figrefdef{compositional-dec}{Compositional decidability}{\agda{compositional-dec}}.\footnote{%
A few explanatory remarks:
\begin{itemize}

\item {\AS Î» \AS{()}} is the unique function from \AD âŠ¥ to any type (here also {\AD âŠ¥}),  sometimes called ``\AF{absurd}'' or ``\AF{âŠ¥-elim}''.

\item \AF{injâ‚} and \AF{injâ‚‚} are left and right injections into a sum type.

\item \AF{projâ‚} and \AF{projâ‚‚} are left and right projections out of a product type.

\item For functions {\AB f \AS : \AB A \AS â†’ \AB C} and {\AB g \AS : \AB B \AS â†’ C}, the function {\AF [ \AB f \AF , \AB g \AF ] \AS : \AB A \AF âŠŽ \AB B \AS â†’ \AB C} maps {\AIC{injâ‚} \AB a} to {\AB f \AB a} and {\AIC{injâ‚‚} \AB b} to {\AB g \AB b}.
% In this use, {\AB C} is \AD âŠ¥, since we are proving a negation.

\item \AF{\_âœ¶â€½} reflects the fact that the empty list exists for every element type (even \AF âŠ¥).

\end{itemize}

}
Moreover, decidability lifts naturally from types to predicates:
\agda{Decidable}
With these definitions, we can formulate the problem of decidable language recognition: given a language \AB{P}, construct a term of type {\AF{Decidable} \AB{P}}.
Since this transformation cannot be fully automated, we will instead look for decidable building blocks that mirror the predicate vocabulary defined in \secref{Specifying Languages}.

Type isomorphisms (such as those in \figref{nu-delta-lemmas}) play an important role in decidability, namely that isomorphic types or predicates are equivalently decidable:
\agda{isomorphisms}
One direction of the isomorphism proves \AB{B} from \AB{A}, while the other proves {\AF Â¬ B} from {\AF Â¬ A}.
In fact, logical \emph{equivalence} suffices for the results in this paper, but the stronger condition of isomorphism enables variations such as generating all parses/proofs rather than just one.

\sectionl{From Languages to Parsing}

\subsectionl{Reflections}

The lemmas in \figref{nu-delta-lemmas} tell us how to decompose languages defined in terms of the vocabulary from \figref{Lang-ops}, while the definitions in \figref{compositional-dec} tell us how compute inhabitation of the resulting types, resulting in decidable parsing.
These lemmas and definitions cannot be applied \emph{automatically} in their present form, however, because languages are functions, as are \AF{Î½} and \AF{Î´}, and so are not subject to pattern-matching.
An automatic solution would need some form of \emph{reflection} of language operations or of \AF{Î½} and \AF{Î´} as inspectable data.

This situation is exactly as in differential calculus, since differentiation in that setting is also defined on functions rather than on symbolic representations.
Fortunately, there are two standard solutions for \emph{computable} differentiation, commonly referred to as ``symbolic'' and ``automatic'' differentiation.\out{\footnote{There is also an incorrect (approximate) variation often referred to as ``numeric'' differentiation.}}
In the former, functions are represented symbolically in some suitable vocabulary, and pattern-matching is used to apply rules of differentiation to those symbolic representations.
The latter solution involves re-interpreting the vocabulary of functions to construct not just the usual functions, but also their derivatives \citep{Griewank89onAD, GriewankWalther2008EvalDerivs, Elliott-2018-ad-icfp}.
This latter option is often much more efficient than the former, since it easily avoids a good deal of redundant computation due to significant amounts of duplicated work between function ``primals'' and their derivatives.

\secreftwo{Symbolic Differentiation}{Automatic Differentiation} applies the symbolic and automatic strategies respectively to languages.
In both cases, the algorithms are automatically proved correct by type checking as corollaries of the lemmas in \figref{nu-delta-lemmas}.

\subsectionl{Symbolic Differentiation}
\rnc\source{Symbolic}

\nc\api{\setstretch{1.1}\agda{api}}

The ``symbolic differentiation'' style reflects language-building vocabulary (\figref{Lang-ops}) into an inductive data type (of modestly extended regular expressions), while \AF{Î½} and \AF{Î´} become functions defined by pattern-matching on that data type.
For convenience, we can use the same names as in \secref{Specifying Languages} for these new counterparts, while referring to the original versions via the module prefix ``{â—‡.}\hspace{0.05em}''.
The result is shown in \figrefdef{symbolic-api}{Regular expressions (inductive)}{\api} along with a function {\AF{âŸ¦\_âŸ§}} that converts a symbolic language representation to a computable parser.

Decidable parsing relies only on \AF{Î½} and \AF{Î´} (via {\AF{âŸ¦\_âŸ§}}), which are defined in \figrefdef{symbolic-defs}{Symbolic differentiation (column-major/patterns)}{\agda{defs}}, as systematically derived from the lemmas of \figref{nu-delta-lemmas}.\out{\notefoot{Describe this derivation.}}
These definitions are meant to be read in ``column-major'' order, i.e., each column is one function definition.
Correctness is guaranteed by the types of \AF{Î½} and \AF{Î´} and so is proved automatically by type-checking \figref{symbolic-defs}.
(We could thus use \emph{any} definitions of \AF{Î½} and \AF{Î´} that type- and termination-check, although not many definitions would.)

In \figref{symbolic-defs}, note the role of the isomorphisms from \figref{nu-delta-lemmas}.
Since those isomorphisms relate \AF{Î½} and \AF{Î´} on the language-building operations to types and languages expressed in that \emph{same} vocabulary, we end up with a recursive algorithm.

\subsectionl{Automatic Differentiation}
\rnc\source{Automatic}

\agda{rules}
\rnc\rulesSep{-3ex}

\secref{Symbolic Differentiation} embodies one choice of reflecting functions into a data representation.
Now consider the dual strategy of ``automatic differentiation''.
This time, reflect \AF{Î½} and \AF{Î´} into a \emph{coinductive} data type of tries\footnote{The classic trie (``prefix tree'') data structure was introduced by \citet{Thue1912Gegenseitige} to represent sets of strings \citep[Section 6.3]{Knuth1998ACP3}, later generalized to arbitrary non-nested algebraic data types \citep{Connelly1995GenTrie}, and then from sets to functions \citep{Hinze2000GGT}.}, while redefining the language-building vocabulary as functions on that data type defined by \emph{copatterns} \citep{AbelPientka2016}.
(This program duality pattern has been noted and investigated by \citet{OstermannJabs2018}.)
Again, we will use the same names as in \secref{Specifying Languages}, referring to the original versions via the module prefix ``{â—‡.}\hspace{0.05em}''.
The result is shown in \figrefdef{automatic-api}{Tries (coinductive)}{\api}.

Decidable recognition again relies only on \AF{Î½} and \AF{Î´}, which are defined in \figrefdef{automatic-defs}{Automatic differentiation (row-major/copatterns)}{\agda{defs}}.
These definitions are meant to be read in ``row-major'' order, i.e., each row is one definition.
Note that the definitions are syntactically identical to those in \figref{symbolic-defs} but are organized dually.
(The compiler-generated syntax coloring differs to reflect the changed interpretation of the definitions.)

The correctness proof is \emph{almost} accomplished by type- and termination-checking, but a technical problem arises.
The {\AF{Î´} (\AB{p} \AF â‹† \AB{q})} clause does not satisfy Agda's termination checker, which cannot see that the argument {\AF{Î´} \AB{p} \AB{a}} in the recursive use of \AF{\_â‹†\_} is in some sense smaller than \AB{p}.
(\figref{automatic-defs} compiles only due to suppressing termination checking for the problematic clause with a compiler pragma.)
Fortunately, this issue was already identified and solved by Andreas \citet{Abel2016}---also in the setting of trie-based language recognition---by using \emph{sized types} \citep{Abel2008, AbelPientka2016}.
\rnc\source{SizedAutomatic}
This solution only requires giving \AF{Lang} (now tries) an index {\AB{i} \AK : \APT{Size}} corresponding to the maximum depth to which a trie can be searched, or equivalently the longest string that can be matched\out{ (via {\AF{âŸ¦\_âŸ§}})}.
In practice, we will work with arbitrarily deep tries, i.e., ones having index \APo{âˆž}, as in the type of {\AF{âŸ¦\_âŸ§}}.
The modified representation is shown in \figrefdef{sized-automatic-api}{Sized tries (coinductive)}{\api}.
Decidable recognition is defined exactly as with unsized tries (\figref{automatic-defs}), with the sole exception of removing the compiler pragma that suppressed termination checking.
This time the compiler successfully proves \emph{total} correctness (including termination).

\rnc\source{Calculus}

To more clearly see the parallel with automatic differentiation (AD), note that AD implementations generally sample a function \emph{and} its derivative together to exploit the fact that these two computations typically share much common work.
This work sharing also applies when differentiating languages.
Without this optimization we have
\agda{ð’Ÿâ€²}
The more efficient, work-sharing version:
\agda{Ê»ð’Ÿ}
Their equivalence follows from lemmas \AF{Î½âˆ˜ð’Ÿ} and \AF{ð’Ÿfoldl} of \secref{Decomposing Languages}:
\agda{ð’Ÿâ€²â‰¡Ê»ð’Ÿ}
The trie representation accomplishes this sharing by weaving \AF{Î½} and \AF{Î´} into a single structure based on common prefixes.


\sectionl{Generalizing from languages to predicates}

\rnc\source{Predicate}

The language-building vocabulary defined in \secref{Specifying Languages} can be simplified and generalized.
First note that there are two categories of operations.
One category transforms the codomain (types): \AF{âˆ…}, \AF ð’°, \AF{\_âˆª\_}, and \AF{\_âˆ©\_}.
The other transforms the domain (lists): \AF{ðŸ}, \AF{\_â‹†\_}, \AF{\_â˜†}, and \AF{`}.
The first category applies to predicates over all types (not just over lists).
Within the second category, \AF{ðŸ}, \AF{\_â‹†\_}, and \AF{\_â˜†} apply to all monoids\footnote{The \emph{definitions} of those operations do not require the monoid properties, but their properties will (\secref{Predicate Algebra}).}, while \AF{`} is specific to lists.

\figrefdef{predicates}{Predicate operations}{
\ifacm
\small
\fi
\begin{center}
\mathindent-12ex
\agda{Pred}
\end{center}
\ifacm
\vspace{1.5ex}
\else
\vspace{-2ex}
\fi
\hfill
\mathindent0ex
\begin{minipage}[b]{19em}
\agda{codomain-transformers}
\end{minipage}
\hfill
\begin{minipage}[b]{19em}
\agda{domain-transformers}
\end{minipage}
\hfill\;
} shows two parallel collections of predicate operations, each covariantly transforming the codomain (left) or domain (right).\footnote{The types of operations on the left can be further relaxed from {\APT{Set} \AB â„“} to any codomain type, while keeping \AB{A} fixed, resulting in the usual covariant applicative functor of functions \emph{from} a fixed type \citep{McBride2008APE}.
Dually, the operations on the right (in their current generality) form a covariant applicative functor of functions \emph{to} types.}
These generalized operations then specialize to language operations as in \figrefdef{lang-via-pred}{Languages via predicate operations}{
\mathindent0ex
\begin{center}
\mathindent-12ex
\agda{Lang}
\end{center}
\ifacm
\vspace{1ex}
\else
\vspace{-2ex}
\fi
\hfill
\begin{minipage}[c]{11em}
\setstretch{1.7}
%% (any domain type)
\agda{codomain-ops}
\end{minipage}
\hfill
\begin{minipage}[c]{15em}
%% (any monoid)
\agda{domain-ops}
%% (lists)
\agda{list-ops}
\end{minipage}
\hfill\;
}, which adds language complement (\AF âˆ).

\rnc\source{Calculus}
The \AF{Î½} and \AF{Î´} lemmas for codomain (but not domain) operations in \figref{nu-delta-lemmas} are easily generalized as shown in \figrefdef{nu-delta-codomain-lemmas}{Properties of \AF{Î½} and \AF{Î´} for predicate codomain operations}{\setstretch{1.5}\agda{Î½Î´-codomain}} and are all proved automatically by normalization.
Since the decidable language implementations in \secref{From Languages to Parsing} are corollaries of \AF{Î½} and \AF{Î´} lemmas, those implementations adapt easily to the generalized codomain operations (\AF{pureáµ€}, \AF{mapáµ€}, and \AF{mapáµ€â‚‚}), resulting in a somewhat smaller implementation and broader coverage.

\sectionl{Properties}

\subsectionl{Predicate Algebra}

The basic building blocks of type-level predicates---and languages in particular---form the vocabulary of a \emph{closed semiring} in two different ways\out{, as reflected in the structure of regular expressions \needcite{}}.
The semiring abstraction has three aspects: (a) a commutative monoid providing ``zero'' and ``addition'', (b) a (possibly non-commutative) monoid providing ``one'' and ``multiplication'', and (c) the relationship between them, namely that multiplication distributes over addition and zero.\footnote{Distribution of multiplication over zero is also known as ``annihilation''.}
In the first predicate semiring, which is \emph{commutative} (i.e., multiplication commutes), zero and addition are \AF âˆ… and \AF{\_âˆª\_}, while one and multiplication are \AF ð’° and \AF{\_âˆ©\_}.
Closure adds a star/closure operation \AF{\_âœ¯} with \AF{starË¡} law: {\AB x \AF âœ¯ \AF â‰ˆ 1 \AF + \AB x \AF âœ² \AB x \AF âœ¯}.

Conveniently, booleans and types form commutative semirings with all necessary proofs already in the standard library.\footnote{The equivalence relation used for types is isomorphism rather than equality.
The boolean semiring is idempotent, while the type semiring is not (in order to distinguish multiple parsings in ambiguous languages).
The latter non-idempotence accounts for the difference between the \AF Î½ and \AF Î´ lemmas for {\AB P â‹† \AB Q} and {\AB P â˜†} in \figref{nu-delta-lemmas} and the corresponding rules in previous work, as mentioned in \secref{Related Work}.}
They are both closed as well.
For booleans, closure maps both \AIC{false} and \AIC{true} to \AIC{true}, with the \AF{starË¡} law holding definitionally.
For types, the closure of \AB{A} is {\AB{A} âœ¶} (the usual inductive list type) with a simple, non-inductive proof of \AF{starË¡}.

This first closed semiring for predicates follows from a much more general pattern.
Given any two types \AB{A} and \AB{B}, if \AB{B} is a monoid then {\AB A \AS â†’ \AB B} is as well.
The monoid operation \AB{\_âˆ™\_} lifts to the function-level binary operation {\AS Î» (\AB{f} \AB{g} : \AB{A} \AS â†’ \AB{B}) \AS â†’ \AS Î» \AB a \AS â†’ \AB f \AB a \AB âˆ™ \AB g \AB a}.
The monoid identity {\AB Îµ \AS : \AB{B}} lifts to the identity {\AS Î» \AB a \AS â†’ \AB Îµ}.
All of the laws transfer from \AB{B} to {\AB A \AS â†’ \AB B}.
Likewise for other algebraic structures.
(As always, proofs are in this paper's source code.)

Looking more closely, additional algebraic structure emerges on predicates: (full) \emph{semimodules} generalize vector spaces by relaxing the associated types of ``scalars'' from fields to commutative semirings, i.e., dropping the requirements of multiplicative and additive inverses.
Left and right semimodules further generalize scalars to arbitrary semirings, dropping the commutativity requirement for scalar multiplication.
Again, nothing is needed from the codomain type \AB{B} beyond the properties of a semiring, so again predicates get these algebraic structures for free (already paid for by types).
For every {\AB{P} \AS : \AF{Pred} \AB{A}}, \AB{P} is a vector in the semimodule {\AF{Pred} \AB{A}}, including the special case of ``languages'', for which \AB{A} is a list type.
The dimension of the semimodule is the cardinality of the domain type \AB{A} (typically infinite), with ``basis vectors'' having the form {\AF{pureâ±½} \AB{a}} for {\AB{a} \AS : \AB{A}}.
A general vector (predicate) is a linear combination (often infinite) of these basis vectors, with addition being union and scaling defined by conjoining membership proofs (as in \figref{lang-via-pred}).

There is still more algebraic structure to be found and exploited.
When our \AB{domain} type \AB{A} is a monoid (as with languages, infinite streams and grids, and functions of continuous space and time), predicates over \AB{A} form a second semiring, known as ``the monoid semiring'' \citep{Golan2005RecentSemi}, in which zero and addition are as in the first predicate commutative semiring and semimodule, while one and multiplication are \AF{ðŸ} and \AF{\_â‹†\_}, defined in \figreftwo{Lang-ops}{lang-via-pred}.
In this setting, language concatenation is subsumed by a very general form of \emph{convolution} \cite{Golan2005RecentSemi, Dongol2016CUC, Elliott2019-convolution}.
The semimodule structure of predicates provides the additive aspect and is built entirely from the \emph{codomain}-transforming predicate operations defined in \figref{Lang-ops} and redefined in \figref{lang-via-pred}.
The multiplicative aspect (convolution) comes entirely from the \emph{domain}-transforming operations applied to any domain monoid.
The two needed distributive properties hold for \emph{any} domain-transforming operation, whether associative or not.
The proofs of these properties, while reasonably straightforward, are beyond the scope of this paper but are available in the paper's Agda source code.\out{\notefoot{There will be short and long forms of this paper. Perhaps the long form will contain narrated parts of the monoid semiring proof.}}

Not only do types form a closed semiring, the \emph{only} properties needed from types in the monoid semiring come from the laws of closed semirings.
One can thus apply the ideas in this paper to many other useful semirings, including natural numbers (e.g., number of parses), probability distributions, and the tropical semirings (max/plus and min/plus) for optimization.
\out{We will explore this generality in \secref{Beyond Predicates}.}
When the domain monoid commutes (multiplicatively)---e.g., for functions of space and time rather than languages---the monoid semiring does as well.
Applications include power series (univariate and multivariate) and image processing \citep{Elliott2019-convolution}.
Research on semirings in parsing began with \citet{Chomsky1959CFL} and was further explored by \citet{Goodman1998PIO, Goodman1999SP} and by \citet{Liu2004}.

\subsectionl{Transporting Properties}

\rnc\source{Transport}

We have seen that type-level languages have useful and illuminating algebraic properties and that the decidable implementations in \secref{From Languages to Parsing} are tightly connected to languages.
Must we prove these properties again for each implementation---here there are more operational details to manage---or do the algebraic properties somehow transfer to those implementations?
This question immediately raises a technical obstacle.
Algebraic abstractions and their properties are simply typed.
For instance, each instance of the \ARe{Monoid} abstraction involves a single type \AB Ï„ with a binary operation {\AB{\_âˆ™\_} \AK : \AB Ï„ \AK â†’ \AB Ï„ \AK â†’ \AB Ï„} and identity value {\AB Îµ \AK : \AB Ï„}.
This recipe accommodates the language operations in \figreftwo{Lang-ops}{lang-via-pred} and their generalizations in \figref{predicates} but not the dependently typed, \emph{indexed} versions of languages and their operations in \figreftwo{symbolic-api}{sized-automatic-api}, nor decidable types and their vocabulary in \figref{compositional-dec}.

Fortunately, there is a simple way to encapsulate indexed types into non-indexed types, namely existential quantification.
For instance, {\AF{âˆƒ} \AD{Dec}} and {\AF{âˆƒ} \AD{(\ARe{Lang} \APo âˆž)}} are non-indexed types.
For a type family {\AB F \AK : \AB A \AK â†’ \APT{Set} \AB â„“}, values of type {\AF âˆƒ \AB F} are pairs {(\AB a , \AB b)} with {\AB a \AK : \AB A} and {\AB b \AK : \AB F \AB a}.
We can also encapsulate dependently typed \emph{operations} on indexed types into simply typed operations on the (simply typed) existentially wrapped versions of those types.
For instance, consider language union from \figref{symbolic-api}:
\begin{code}
% Copied from latex/Symbolic.tex
\>[2]\AgdaOperator{\AgdaInductiveConstructor{\AgdaUnderscore{}âˆª\AgdaUnderscore{}}}%
\>[7]\AgdaSymbol{:}\AgdaSpace{}%
\AgdaDatatype{Lang}%
\>[15]\AgdaGeneralizable{P}%
\>[18]\AgdaSymbol{â†’}\AgdaSpace{}%
\AgdaDatatype{Lang}\AgdaSpace{}%
\AgdaGeneralizable{Q}%
\>[28]\AgdaSymbol{â†’}\AgdaSpace{}%
\AgdaDatatype{Lang}\AgdaSpace{}%
\AgdaSymbol{(}\AgdaGeneralizable{P}%
\>[39]\AgdaOperator{\AgdaFunction{â—‡.âˆª}}%
\>[44]\AgdaGeneralizable{Q}\AgdaSymbol{)}\<%
\end{code}
Referring to the type and operations of \figreftwo{Lang-ops}{symbolic-api} via the module prefixes ``{â—‡.}\hspace{0.05em}'' and ``{â–¡.}\hspace{0.05em}'' respectively, define \agda{wrapped-Lang}
Then wrap union as follows:
\agda{wrapped-union}
The type-level language components of the arguments and result are given explicitly here but could instead be inferred by the compiler.
\rnc\source{Existential}
Indexed operations of all arities for a type family \AB{F} can be systematically wrapped in this style, as shown in
\figrefdef{inj}{Existential wrappers}{\mathindent0ex\agda{inj}}, again with automatically inferable components given explicitly for clarity.
Then wrapped union can be defined simply as {\AF{\_âˆª\_} \AS = \AF{injâ‚‚} \AIC{â–¡.\_âˆª\_}} and likewise for the other operations.

In addition to transporting types and operations as just described, we also need to transport \emph{properties}.
A crucial question is the choice of equivalence relation for the new properties.
If we choose equivalence on these dependent pairs to be equivalence of \emph{first} projections (type \emph{indices}), then all algebraic properties of those indices will hold for the existentially wrapped versions as well.
This choice may at first seem like cheating, since the entire operational aspect of the representation is ignored.
The correctness of that aspect, however, is guaranteed by its dependent typing, which anchors its meaning (no matter how cleverly implemented) to the non-operational type index.
For the language implementations of \secref{From Languages to Parsing}, this anchoring is guaranteed by the type of \AF{âŸ¦\_âŸ§} in \figreftwo{symbolic-api}{sized-automatic-api}.
(Similarly, for the decidable type constructors in \figref{compositional-dec}, correctness is guaranteed by the types of the \AIC{yes} and \AIC{no} constructors in the definition of â€Œ\ARe{Dec} in \secref{Decidability}.)
Given this choice of equivalence, properties of index types easily lift to properties of existential types, as shown in \figrefdef{prop}{Liftings of index properties to existential types}{\mathindent0ex\agda{prop}}.
(These definitions are partially inferable as well, e.g., from \agda{propâ‚ƒâ€²}.)
These definitions ease specification of algebraic instances for existentially wrapped types from corresponding instances for their indices.
\rnc\source{Transport}
As examples, we can lift the commutative semiring of types to wrappings of the decidable counterparts from \figref{compositional-dec} in \secref{Decidability}, and lift both language semirings to the decidable implementations in \secref{From Languages to Parsing}, as in \figrefdef{transport-examples}{Examples of algebraic instances transported from indices to existential wrappings}{\agda{examples}}.


\sectionl{Related Work}

Most work on formal languages in functional programming has been in parsing, particularly via parsing combinators \citep{Hutton1996monadic, Leijen2001parsec, Swierstra2008combinator}.
Such work typically concentrates on usability, composability, and sometimes efficiency, rather than on simple semantic specification and verifiable correctness.

There has also been some work using type theory to capture languages as type-level predicates much like in \secref{Specifying Languages}:
\begin{itemize}

\item \citet{AgularMannaa2009} also defined a non-indexed algebraic type of regular expressions.
They also \emph{defined} indexed types for \AF{Î½} and \AF{Î´} on regular expressions rather than defining the meanings of those operations in terms of languages and then proving properties of them.
They proved a consistency property about \AF{Î´}, but apparently not about \AF{Î½}, and they note their method's ``inability to prove the non-membership of some string in a given language''.

\item \citet{DoczkalEtAl2013} formalized regular languages constructively in Coq, also in terms of a type-level membership predicate.
They then proved several classic results about finite automata, including minimization and relationship to Nerode and Myhill partitions.

\item \citet{FirsovUustalu2013} used Agda to formalize regular expressions with a corresponding inductive type of language membership proofs and then related regular expressions to nondeterministic finite automata (NFAs) represented as boolean matrices, as well as operations on that representation that correspond to those on regular expressions.
They then proved soundness and completeness of the NFA representation with respect to regular language membership.

\item \citet{KorkutEtAl2016} defined a non-indexed algebraic data type of regular expressions together with an indexed inductive type of proofs of membership of strings in the language denoted by the regular expressions and a corresponding function that matches an implicitly concatenated stack of regular expressions.
Special attention was paid to ensuring and proving termination, particularly in regard to languages that contain the empty string.
Addressing both involved defunctionalization and translating between general regular expressions and a restricted ``standard'' form.

\item \citet{Traytel2017} defined formal languages as tries, with operations via primitive corecursion and reasoning via coinduction, all in the Isabelle/HOL proof assistant.
There appears to have been no specification higher level (less operational) than tries, leading to some rather complex definitions (which, as \emph{definitions}, cannot be proved correct).
Language concatenation was particularly delicate.
For the same reason, proofs of even basic properties involved coinduction.
The authors pointed out, however, that the coinductive trie formulation enabled better proof automation than with sets of strings.

\item \citet{Abel2016} also formulated languages via tries and in particular noted the termination issue mentioned in \secref{Automatic Differentiation} and provided the sized-types solution adopted in this paper.
Relationships to automata theory and coalgebraic notions were also explored in depth.
Trie look-up yielded boolean values rather than propositions, and there was no simpler specification of languages with respect to which the trie implementation could be specified and proved consistent, so every operation was \emph{defined} coinductively.
The concatenation and closure (star) cases were especially delicate.
Equality on tries was defined by strong bisimilarity, and thus properties required corresponding machinery to state and prove.

%% requiring more careful construction and reading than the specification given in \figref{Lang-ops} and used in the implementations in \secref{From Languages to Parsing}

\item \citet{BaanenSwierstra2020} also explored correctness of regular expression matching in Agda, but from the perspective of \emph{effects} and their predicate transformer semantics.
The authors shared a common starting point with \citet{KorkutEtAl2016} and addressed differentiation on regular expressions, while separating termination from partial correctness.
Their regular expression matching function targets a free monad (later to be interpreted with suitable effect semantics) rather than propositions.

\end{itemize}

Of the previous work involving language derivatives, none appear to have based their formal specification and proofs on the essential, non-inductive, nearly-trivial definition in terms of functions of lists from \secref{Decomposing Languages}, although \citet{Abel2016} did indeed mention this definition informally as motivation for the more complex, coinductively defined operations on tries.
\citet{Brzozowski64} also made clear mention of the underlying meaning on sets of strings in his original work on regular expression derivatives.
Moreover, none of these previous investigations appear to have addressed proof isomorphism (distinguishing multiple parsings/explanations), and correspondingly all use transformations based the less precise laws originally discovered by \citet{Brzozowski64}.
Those laws were based on the assumption that union (``addition'' in both language semirings of \secref{Predicate Algebra}) is idempotent, which is not true for type-level language predicates related by proof isomorphism (rather than mere logical equivalence).
The lack of idempotence is crucial for going beyond recognizers to parsers, in which we might want to extract more than one proof (parsing).
The difference is mainly visible in the \AF Î½ and \AF Î´ lemmas for {\AB P â‹† \AB Q} and {\AB P â˜†} in \figref{nu-delta-lemmas}.

\bibliography{bib}

\end{document}
